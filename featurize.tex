%-*- mode: LaTex; outline-regexp: "\\\\section\\|\\\\subsection";fill-column: 80; -*-
\documentclass[12pt]{article}
\usepackage[longnamesfirst]{natbib}
\usepackage[usenames]{color}
\usepackage{graphicx}  % Macintosh pdf files for figures
\usepackage{amssymb}   % Real number symbol {\Bbb R}
\usepackage{amsmath}
\usepackage{bbm}
\input{../../standard}

% --- margins
\usepackage{../sty/simplemargins}
\setleftmargin{1in}   % 1 inch is NSF legal minimum
\setrightmargin{1in}  % 1 inch is NSF legal minimum
\settopmargin{1in}    % 1 inch is NSF legal minimum
\setbottommargin{1in} % 1 inch is NSF legal minimum

% --- Paragraph split, indents
\setlength{\parskip}{0.00in}
\setlength{\parindent}{0in}

% --- Line spacing
\renewcommand{\baselinestretch}{1.5}

% --- page numbers
\pagestyle{empty}  % so no page numbers

% --- Hypthenation
\sloppy  % fewer hyphenated
\hyphenation{stan-dard}
\hyphenation{among}

% --- Customized commands, abbreviations
\newcommand{\TIT}{{\it  {\tiny Featurizing Text (DRAFT, \today)}}}
\newcommand{\prs}{\mbox{$\ol{\ol{R}}^2$}}

% --- Header
\pagestyle{myheadings}
\markright{\TIT}

% --- Title

\title{ Featurizing Text: Converting Text into Predictors for
            Regression Analysis }
\author{
        Dean P. Foster\footnote{Research supported by NSF grant 1106743} 
        \ \ Mark Liberman 
        \ \ Robert A. Stine\footnotemark[\value{footnote}]   \\
        Department of Statistics                             \\
        The Wharton School of the University of Pennsylvania \\
        Philadelphia, PA 19104-6340                          
}

\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle 
%------------------------------------------------------------------------
\vspace{-.5in}
\abstract{  

 Modern data streams routinely combine text with the familiar numerical data
 used in regression analysis.  For example, listings of real estate that show
 the price of a property typically include a verbal description.  Some
 descriptions include numerical data, such as the number of rooms or the size of
 the home.  Many others, however, only verbally describe the property, often
 written in an idiosyncratic vernacular.  For modeling such data, we describe several
 methods that that convert  text into numerical features suitable for
 regression analysis.  The proposed featurizing techniques create regressors
 directly from text, requiring minimal user input.  The techniques range naive
 to subtle.  One can simply use raw counts of words, obtain principal components
 from these counts, or build regressors from counts of adjacent words.  Our
 example that models real estate prices illustrates the surprising success of
 these methods.  To partially explain this success, we offer a motivating
 probabilistic model.  Because the derived regressors are difficult to
 interpret, we further show how the presence of incomplete quantitative features
 extracted from text can illuminate the structure of a model.

}

%------------------------------------------------------------------------
\vspace{0.15in}

\noindent
{\it Key Phrases:  latent semantic analysis, random projection, singular value decomposition, text mining, topic models} 

\clearpage

% ----------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}
% ----------------------------------------------------------------------


\begin{description}


\item{Relate to topic models} \\ 
 Simulation

\item{Variable selection}  \\
 AIC and connection to CV analysis.  Plot of reflections.

\item{Discussion}  \\
Lighthouse variables combine regular expression variables and linguistic variables. Other applications to wine.

\end{description}

 
 Software packages, such as (very elaborate, comprehensive) those in R
 \citep[tm][]{feinerer08} or Stanford NLP or openNLP.

 stopwords... ``and'', ``for'', ``in'', ``is'', ``it'', ``not'', ``the'', ``to'')
 
\ras{Would sparse CCA (Witten et al) be useful. \citet{witten09}}
 
 \clearpage


 Modern data streams routinely combine text with numerical data.  For example, patient medical records combine lab
 measurements with physician comments.  Analogously, online product ratings
 shown at Amazon blend explicit characteristics with verbal commentary.  How can
 one easily incorporate the information in such text into a model
 that predicts patient outcomes or product ratings?  Our objective here is to
 describe several particularly straightforward techniques for featurizing
 text, turning text into numerical variables suitable for regression modeling.
 
 
 As a specific example, we build a regression that predicts advertised prices
 of real estate properties from their listings.  Our data describe $n=$7,384
 property listings for Chicago, IL, extracted (with permission) from trulia.com
 on June 12, 2013.  At the time, trulia.com showed 30,322 listings for Chicago,
 but most were foreclosures that we excluded from our analysis.  The response in
 our models is the log of the listed price.  The distribution of the listed
 prices is right skewed, so we transformed the response to a log scale as shown
 in Figure \ref{fig:prices}.  The log transformation produces a roughly Gaussian
 distribution, with a hint of fat tails for produced by inexpensive properties
 with prices near \$25,000.  (This display uses the base 10 log of the prices
 for easy interpretation; subsequent models use natural logs.)
 
 
 \begin{figure}
 \caption{ \label{fig:prices} { \sl The distribution of prices for real estate
 in Chicago is highly skewed.  A log transformation produces data that are
 nearly normal.}  }
 \centerline{
 \vspace{0.1in}
 \includegraphics[width=5in]{figures/prices} }
 \vspace{0.2in}
 \end{figure}


 The listings that describe these properties are verbal, written in an
 idiosyncratic vernacular familiar only to those who are house hunting.  Some
 listings contain quantitative characteristics, but many do not.  The 
 following four listings are typical.  The initial value is the listed price.

 \begin{verbatim}
    $399000 Stunning skyline views like something from a postcard are yours
    with this large 2 bed, 2 bath loft in Dearborn Tower!  Detailed
    hrdwd floors throughout the unit compliment an open kitchen and
    spacious living-room and dining-room /w walk-in closet, steam
    shower and marble entry.  Parking available. 

    $13000 4 bedroom, 2 bath 2 story frame home. Property features a
    large kitchen, living-room and a full basement. This is a Fannie Mae
    Homepath property. 

    $65000 Great short sale opportunity...  Brick 2 flat with 3 bdrm
    each unit. 4 or more cars parking. Easy to show. 

    $29900 This 3 flat with all 3 bed units is truly a great
    investment!! This property also comes with a full attic that has
    the potential of a build-out-thats a possible 4 unit building in a 
    great area!!  Blocks from lake and transportation. Looking for a
    deal in todays market - here is the one!!! 
 \end{verbatim}

 \noindent
 Listings do not obey the grammatical rules of English.  Some authors write in
 sentences, others not, and a variety of abbreviations appear.  Punctuation varies from spartan to effusive (particularly exclamation marks),
 and the length of the listing runs from several words to a long paragraph.  The
 average listing has 73 words. The distribution of the lengths is right
 skewed; the shortest listing has 2 words whereas the longest has 568 words.
  This variation in the lengths of the descriptions suggests that modeling the
 prices from this text requires some form of weighting: we simply do not
 know much about properties with short descriptions.

  
 A natural approach for a statistician confronted with such data is to construct
 intuitively appealing regressors from the listings.  For example, one might
 conjecture that an agent has a lot more to say about an expensive property than
 one in need of repair.  Hence, the length of a listing 
 may be predictive of its price.  In a different vein, one can use regular
 expressions -- pattern matching -- to extract specific numerical data from the
 listings.  For example, the first listing indicates that this property has two
 bedrooms and two bathrooms.  A regular expression can be designed to extract
 numbers that precede the word ``bath'' from the listings.  Constructing such
 regular expressions, however, is a labor-intensive process that must be done on
 a case-by-case basis.  The patterns become complex because they must allow
 common abbreviations, such as ``bth'', ``bath'' and ``bthrm'' in order to match
 counts of the number of bathrooms.  Complexity aside, the greater problem with
 explicit pattern matching is that most listings omit these characteristics.
  The only numerical data common to every listing is the response, the
 advertised price.  For these listings, our regular expressions found that 6\%
 of the listings indicate the number of square feet, 26\% indicate the number of
 bathrooms, and 42\% give the number of bedrooms.  More complex regular
 expressions would likely find more matches, but the gains are likely to be few.
  As illustrated next, one faces a Type I/Type II trade-off.  Simple
 regular expressions miss some matches, but more aggressive expressions match
 inadvertently.


 The four scatterplots in Figure \ref{fig:parsed} summarize the marginal
 association between the log of prices and these constructed predictors,
 including the number of words in descriptions.  For listings with no match, we
 filled the missing values with the averages of the observed cases.  These
 appear as columns of gray points located at the mean of each variable on the $x$
 axes in Figure \ref{fig:parsed}.  The correlations between these variables and
 the log of the prices vary from moderate to nearly zero.  The length of the
 listing has the largest correlation with log prices ($r=0.40$). The frame that shows this association includes the fit of
 a fifth-order polynomial that recovers the evident nonlinearity of this
 association.  The nonlinearity is highly significant, but increases the amount
 of explained variation from 0.16 to only 0.20.  The improvement in fit is
 slight because most of the data lie within the linear range of the fit.
  Missing data has a large impact on the correlations with the other extracted
 features shown in Figure \ref{fig:parsed}.  The correlations for complete cases
 are 0.42 for the number of bathrooms, 0.26 for the log of the number of square
 feet, and 0.09 for the number of bedrooms.  If missing data are included, the
 correlations become much smaller (the second correlation shown in each frame).
  These plots also show several anomalies.  For example, the scatterplot of the
 log price on log length shows a cluster of 25 listings, all with exactly 265
 words (log 265=5.6).  These are not errors: All of these different properties
 were listed with a common format by a government agency.  The scatterplot of
 the log of prices on the log of the square footage also shows a clear outlier;
 this outlier is a consequence of aggressive matching in a regular expression.
  A typo in a description (``has 1sfam'') led our regular expressions to find a
 property with 1 square foot.

 \begin{figure}
  \caption{ \label{fig:parsed} { \sl Extracted characteristics have moderate to
 slight positive association with the log of the prices. } Gray points in the
 figures identify cases that omit the explanatory variable; the first shown
 correlation in each frame uses only the observed data; the following smaller 
 value includes mean-filled listings. }
 
\centerline{
 \vspace{0.1in}
 \includegraphics[width=5in]{figures/parsed} }
 \vspace{0.2in}
 \end{figure}


 Table \ref{tab:parsed} summarizes the fit of a regression of the log of price
 on predictors built from the four extracted features. The model includes a
 fifth degree polynomial in the log of the length of the description and missing
 data indicators.  These indicators are coded as 1 when the associated regular
 expression found a match and are coded as 0 otherwise.  Aside from the
 components of the polynomial, the features are not highly collinear, and the
 multiple regression echoes the marginal associations observed in Figure
 \ref{fig:parsed}.  Features related to the lengths of listings explain the most
 variation, followed by the number of bathrooms.  Unlike the marginal
 associations, the log of the number of square feet and its missing indicator
 are highly significant.  The missing indicators for bedrooms and bathrooms are
 not.  The model explains about one-fifth of the variation in prices;  
 if adjusted for prediction, the predictive R-squared \citep{fosterstine14b} is
 \begin{equation}
   \prs = 1 - \frac{\mbox{RSS}/(n-2p)}{\mbox{TSS}/(n-1)} = 0.231.
 \end{equation}
 \prs adjusts for the effects of estimation when predicting out-of-sample.

 
 \begin{table}[ht]
\caption{ \label{tab:parsed} {\sl OLS multiple regression of log prices on the
 parsed explanatory variables and  indicators of observed values.}}
\centering
\begin{tabular}{lrrrr}
  \hline
    Feature       & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
  Intercept       & 7.9985 & 0.4749 & 16.84 & 0.0000 \\ 
  log Tokens      & 39.0183 & 1.0945 & 35.65 & 0.0000 \\ 
  $(\mbox{log Tokens})^2$   & 1.0086 & 1.0642 & 0.95 & 0.3433 \\ 
  $(\mbox{log Tokens})^3$ & -18.2556 & 1.0612 & -17.20 & 0.0000 \\ 
  $(\mbox{log Tokens})^4$ & -4.8956 & 1.0594 & -4.62 & 0.0000 \\ 
  $(\mbox{log Tokens})^5$ & 7.9094 & 1.0584 & 7.47 & 0.0000 \\ 
  log Sq Feet     & 0.4039 & 0.0579 & 6.98 & 0.0000 \\ 
  Sq Feet obs     & 0.6254 & 0.0594 & 10.53 & 0.0000 \\ 
  Bedrooms        & 0.0080 & 0.0165 & 0.48 & 0.6299 \\ 
  Bedrooms obs    & -0.0084 & 0.0299 & -0.28 & 0.7783 \\ 
  Bathrooms       & 0.4015 & 0.0300 & 13.37 & 0.0000 \\ 
  Bathrooms obs   & -0.0199 & 0.0336 & -0.59 & 0.5537 \\ 
   \hline
   \multicolumn{5}{c}{\prs = 0.231, \; $s_e = 1.06$}\\
   \hline
\end{tabular}
\end{table}

 
 
 Had listings been composed in a consistent manner with a standardized set of
 characteristics, such constructive modeling would surely be more successful.
  Our goal here takes us in a different direction that, in contrast to this
 constructive approach, explores automatic methods that thrive in this
 unformatted, irregular context.  These methods are collectively known as {\em
 vector space models} in computational linguistics \citep[e.g.][]{turney10}.  A
 vector space model (VSM) characterizes each document as a point in some
 $p$-dimensional vector space, typically \Rp.  VSMs originated in artificial
 intelligence and were adopted for text applications to construct a system for
 document retrieval known as latent semantic indexing \citep{deerwester88}.
  Latent semantic indexing generates clusters of documents to enable a query to locate
 related items.  VSMs are closely related to techniques that statisticians will
 find familiar: principal components analysis (PCA) and canonical correlation
 analysis (CCA).  The underlying computational engine is a singular value
 decomposition (SVD), aided by random projections.  (The use of the SVD
 occasionally leads to VSMs being described as ``spectral algorithms'' which
 should not be confused with frequency domain methods for time series.)  There
 has been considerable debate as to how VSMs represent language; we leave to
 linguistics the task of explaining why such simple representations of text
 might capture deeper meaning \citep{deerwester90, landauer97, bullinaria07,
 turney10}.


 VSMs are commonly found in unsupervised applications such as clustering, but
 our application here finds that they are equally effective in supervised
 models.  In essence, each document is represented as a point in \Rp, and these
 coordinates become regressors.  These regressors can be used alone or in
 combination with traditional variables, such as those obtained from a lexicon,
 regular expression, or semantic model.  The example of real estate listings
 illustrates the impact of various choices on the predictive accuracy.  For
 example, a regression using the automated features produced by a simple VSM
 explains over two-thirds of the variation in listed prices for real estate in
 Chicago.  The addition of several substantively derived variables adds little.
  Though we do not emphasize its use here, variable selection can be employed to
 reduce the ensemble of regressors without sacrificing predictive accuracy.  An application that models personality traits derived from Facebook
 messages appears in \citet{ungar13}.


 The remainder of this paper develops as follows.  The next section reviews
 latent semantic analysis (LSA) and uses this well-known method from
 computational linguistics to featurize text.  This method corresponds to
 principal components regression.  Section \ref{sec:cca} presents vector space
 models such as LSA from a novel perspective that emphasizes the statistical sense 
 of ``observations,'' samples drawn from a population.  LSA is seen to
 correspond to a CCA between words and the context of those words.  Various definitions of the
 context of a word allow one to construct many types of features.  Though
 embarrassingly simple to describe, it is more subtle to appreciate why a
 VSM works in regression.  Section \ref{sec:topic} offers one
 explanation by relating VSMs to a topic model that reproduces many aspects of the real estate listings.  Section \ref{sec:cv} returns to regression models for real
 estate with a discussion of the use of variable selection  and uses
 cross-validation to measure the success of methods and to compare several
 models.  Variable selection is particularly relevant if one chooses to search
 for nonlinear behavior.  We close in Section \ref{sec:disc} with a discussion
 and collection of future projects.



%--------------------------------------------------------------------------
\section{Featurizing with Latent Semantic Analysis}
\label{sec:lsa}
%--------------------------------------------------------------------------

 LSA is essentially principle components analysis
 of a matrix whose elements count how often each word appears in a document (in our
 case, the frequency of words in a property listing).  Using these components as
 features in regression then amounts to a principle components regression.
  Quite a few details, however, need to be resolved in any application. For
 example, what is a word?  How should the data be scaled? 
 This section describes the relevant choices within
 the context of building a regression model for real estate prices.
 
 \subsection{ Tokenization }  % ------------------

 LSA begins by converting the source text into {\em word tokens}.  A word token
 is a sequence of one or more characters that represents an instance of a unique
 {\em word type}.  This conversion of text into tokens is known as tokenization.
  Tokenization requires making numerous, often subtle, choices.  For example, is
 the number ``735'' a word token?  Do the tokens ``state'' and ``State''
 represent the same word type?  Are ``room'' and ``rooms'' instances of the same word
 type?  To answer these questions, we adopt a standard, simple approach: we
 convert all text to lower case, distinguish punctuation characters as separate
 types, and replace instances of rare word types by a common token.  To
 illustrate some of the issues, consider the following listing:
 \begin{verbatim}
   Brick flat, 2 bdrm.  With two-car garage. \end{verbatim} 
 \noindent
 Separated into tokens, this text becomes a list of 10 tokens representing 9
 word types (angle brackets surround punctuation tokens)
 \begin{verbatim}
   {brick, flat, <,>, 2, bdrm, <.>, with, two-car, garage,<.> } \end{verbatim} 
 \noindent
 We leave embedded hyphens in place and do not correct spelling errors and
 typos.  Abbreviations remain as given.  References such as the texts by
 \citet{manning99} and \citet{jurafsky09} describe more elaborate choices, such
 as stemming and annotation, that can be incorporated into
 tokenization. \citet{turney10} give a concise overview.  Notice that
 tokenization only distinguishes word types, not  meaning or use
 (does not distinguish homographs).

  
 When applied to our data from Chicago, the 7,384 property listings contain
 536,485 word tokens representing 15,227 unique word types.  As usual in text,
 the most common word types occur frequently whereas most words appear
 infrequently.  More than half of these tokens appear only once or twice,
 providing little exposure to how the word is used.  We clustered these rare
 types into one category ($<$OOV$>$, for ``out of vocabulary''), resulting in a
 reduced vocabulary of $m = 5,707$ word types.  The most common word types are
 punctuation: `.'  occurs 40,227 times and `,' occurs 33,746 times.  Following
 these come the collection of seldom seen word types (OOV, 11,478), `and'
 (frequency 11,032), `!' (7,894) and `in' (7,418).  Figure \ref{fig:zipf} graphs
 the log of the frequency of these word types versus the log of their ranks.  It
 is common in text to find a linear trend with slope near 1 in this graph, the
 signature of a Zipf distribution \citep{zipf35, baayen02}.  In that case, the
 rank of a word type times its frequency would be constant.  Even though the
 text of real estate listings is not standard English, one expects to find
 counts resembling those produced by a power law \citep{clauset09}.  The shown
 line (with log-log slope -0.927) holds only for more common word types.  For
 less common words (here, outside the most common 500 words), the frequencies
 drop off more rapidly.  (This change in slope is also seen for words in
 Wikipedia.)


 \begin{figure}
 \caption{ \label{fig:zipf} { \sl The log of the frequency of word types in the
 listings for properties in Chicago is roughly linear in the log of the rank of
 the words.}  The shown line $\log \mbox{freq} = 10.9 - 0.93 \log \mbox{rank}$ 
 was fit to the most common words with ranks 1 through 500.  }

 \centerline{
 \vspace{0.1in}
 \includegraphics[width=3.5in]{figures/zipf} }
 \vspace{0.2in}
 \end{figure}


 Once the source text has been tokenized, we collect counts of how often each
 type appears within a listing.  These counts treat each listing as a ``bag of
 words,'' a multiset that does not distinguish the placement of words within a
 listing.  Permuting the words within a listing  produces the same
 counts.  Let $n$ denote the number of listings; we treat each listing as an observation
 in the usual sense.  Let $V$ denote the vocabulary, the set of
 $m$ distinct word types.  The $n \times m$ context-word matrix $W$ holds the counts of  
 these word types across the listings; $W_{ij}$ is the number of times word type $j$
 appears in the $i$th document.  (All vectors in our notation are column
 vectors.)  (The transpose of $W$, often called the term-document matrix, is common
 within computational linguistics. We follow the convention within statistics
 and arrange observations -- the listings -- as rows. In our context a listing is a document.)  The matrix
 $W$ is sparse: a small portion of the vocabulary appears in most listings. We assume that the columns of $W$ are sorted so that the first column $W_1$ holds the most frequent word type, the second column $W_2$, holds the second most frequent, and so forth.


\subsection{ Regressing on Word Types }  % ----------------------

Before turning to LSA, we fit models directly on the frequencies in $W$.  Because LSA constructs features that are linear combinations of the columns of $W$, all of the signal captured by LSA lies within $W$.  By regressing log prices directly on $W$, we can bound how much variation LSA can explain.  Our fits of log price on $W$ adjust for the lengths of the listings.  As shown in Figure \ref{fig:parsed}, the length of a listing is correlated with its price.  Keeping this polynomial in length in the regression implies that, for a word to be a significant predictor, its association with price must run deeper than its incremental contribution to the length.  Starting from this initial polynomial, we accumulated the $R^2$ statistic as first 3,000 columns of $W$ join the model.  The solid curves in Figure \ref{fig:cumr2} track $R^2$ as these frequencies are added in either decreasing order (upper curve, for adding $W_1,W_2,\ldots,W_{3000}$) or increasing (lower curve, for $W_{3000}, W_{2999},\ldots, W_1$) frequency.  The gap between these curves and concave shape of the upper curve show that adding more frequent words first achieves a better fit with fewer variables. This figure also shows the degree of collinearity within $W$.  PCA is useless if applied to orthogonal variables. The sparsity of $W$ produces small correlations between most columns of $W$, but colinearity emerges within larger collections.  For example, when adding features in order of decreasing frequency, 219 of these 3,000 word types were redundant (representable as linear combinations of others).  To illustrate the effects of this collinearity in regression, the dashed curve in Figure \ref{fig:cumr2} shows the contribution to $R^2$ of words in order of decreasing frequency, but with the magnitude of the change in $R^2$ as if added in the other order. Were there no collinearity, the dashed curve would lie on top of the upper solid curve in Figure \ref{fig:cumr2}.  For example, when added after $W_1,W_2,$ and $W_3$,  the count of OOV tokens ($W_4$) boosts $R^2$ by about 0.02.  If added after $W_{3000},W_{2999}, \ldots, W_5$, the frequency of OOV tokens adds much less to the fit, contributing only 0.0003 to  $R^2$.  

\begin{figure}
\caption{  \label{fig:cumr2}  
  {\sl Accumulated $R^2$ statistics of regression models with word frequencies added in order of decreasing (top solid black) or increasing (lower solid black) frequency.} The dashed curve shows the contributions having adjusted for subsequent words and illustrates the effects of collinearity among the word counts.}  
  \centerline{ \includegraphics[width=4in]{figures/cumr2.pdf} }
\end{figure}


The slow growth of $R^2$ after the first hundred or so frequencies suggests that later features add too much noise to predictions and are not helpful for prediction.  Even so, a model that simply adds the 3,000 most frequent words to the baseline polynomial provides a statistically significant improvement over the initial polynomial in the log of the length of the listings. Adding $W_1,\ldots,W_{3000}$ improves $R^2$ from 0.194 to 0.832, a highly significant increase.  Nonetheless, the resulting model is huge relative to the number of observations and has many insignificant estimates.  One has numerous approaches to trimming its size, such as shrinkage or subset selection.  Because the word frequencies are correlated but easily ordered (by frequency), we use the corrected AIC statistic \citep{hurvich89}
\begin{equation}
    AIC_{c}(k) = n \log \frac{RSS(k)}{n} + \frac{n+k}{1-(k+2)/n} \;,
\end{equation}
where $k$ is the number of estimated parameters in the regression.  Rather than try to pick the best subset -- which would require a strong penalty against over fitting -- we use $AIC_c$ to pick the best stopping point in the single sequence of models determined by word frequency, much as one might pick the order of an autoregression.  Figure \ref{fig:aic} graphs $AIC_c$ for the sequence of models obtained by adding words in order of decreasing frequency when added to the initial polynomial in the log of the length of the listings.   The minimum of $AIC_c$ occurs with $k=1,089$ words, giving \prs = 0.580.   Residual plots show fat tails (consistent with the distribution of log prices in Figure \ref{fig:prices}), but no evidence of heteroscedasticity
 that might be anticipated due to the differences among the lengths of the listings.

\remark{ The comparison of these models explains our preference for reporting \prs in the context of large regressions.  We desire a model to predict the price of a listing from its text.  For the regression with 1,089 words, the more familiar $R^2 =  0.705$ and $\ol{R}^2=0.654$.  The final model summarized with 3,000 words reaches $R^2 =  0.832, \, \ol{R}^2  = 0.719$.  Superficially, then, both $R^2$ and $\ol{R}^2$ convey the impression that this very large model is more predictive than the model chosen by $AIC_c$.  Predictive R-squared provides a more realistic evaluation of the ability of the large model to predict, namely its \prs = 0.15.}


\begin{figure}
\caption{  \label{fig:aic}  
  {\sl Corrected AIC for two regression models: one adds words in order of decreasing frequency (dots), and two that add predictors computed from either LSA (solid) or bigrams (dashed).}  Small circles mark the positions of the minimum $AIC_c$ statistic for each sequence.}
  \centerline{ \includegraphics[width=4in]{figures/aic.pdf} }
\end{figure}


\begin{figure}
\caption{  \label{fig:wordtstats}  
  {\sl Absolute $t$-statistics from the regression of log prices on word frequencies for the model selected by $AIC_c$.}  In the left panel, the horizontal black line locates $\ev |Z|$ for $Z \sim N(0,1)$,  the higher dashed line is the Bonferroni threshold, and the red curve is a loess smooth of $|t|$.  In the right panel, the red line is fit to the smallest 20\% of the $|t|$-statistics, producing the shown slope.}
  \centerline{ \includegraphics[width=5in]{figures/word_tstats.pdf} }
\end{figure}

 
Although the model selected by $AIC_c$ explains a great deal of variation in prices, the signal remains diffusely spread over the words.  Few estimates stand out, leaving substantial signal embedded in the background.   Figure \ref{fig:wordtstats} summarizes the t-statistics in this fit. The left panel of the figure graphs $|t|$ for the words in order of decreasing frequency.  The solid horizontal black line is the expected value of the absolute value of a standard normal, $\sqrt{2/\pi}$. The dashed horizontal line is the Bonferroni threshold $\Phi^{-1}(1-0.025/1089) \approx 4.08$.  Between these, the almost flat red curve is the loess smooth of  $|t|$; the average $|t|$ is only slightly larger than one expects for a model with no signal.   Scattered coefficients of only 16 words exceed the Bonferroni threshold.  For example,
 the coefficient for the word ``vacant'' has $t=-8.1$; not  surprisingly, the
 addition of this word to a listing lowers the expected price.  In
 contrast, the presence of additional out-of-vocabulary words
 suggest with higher than average prices ($t=5.0$).   Though these stand out, a model limited to these clearly significant words has $R^2 = 0.236$, leaving much variation unexplained. 
  The half-normal plot in the right panel of
 Figure \ref{fig:wordtstats} confirms the diffuse signal: the fitted $t$-statistics are inconsistent with noise, but not by much.  The dashed line in the half-normal plot is the diagonal; the red
 line is a  regression fit to the smallest 20\% of the $|t|$-statistics.  For a sample from a standard normal distribution, the slope should be 1.  The slope of these estimates is significantly larger, but clearly the signal is
 widely spread over many words rather than concentrated in a few.
   

 \subsection{ Regressions using LSA }  % ------------------

The use of LSA to create features for regression models essentially reproduces principal components regression.  Rather than regress prices on column of $W$, regress prices on principal components of $W$.  The only differences are the absence of centering and the variety of approaches for weighting counts prior to constructing the principal components.   

For example, one might choose to stabilize the variance of the counts.  Define ${W}^{*}$ to be the $n \times m$ matrix with elements ${W}^{*}_{ij} = W_{ij}/\sqrt{n_i}$ with $n_i = \sum_j W_{ij}$ counting the number of word tokens in the $i$th listing.  Were tokens within listings random samples from
 a multinomial distribution with probabilities $p_{1:m}$ across the word types, then $\Var({W}^{*}_{ij}) = p_j(1-\sum_{k\ne j} p_k)$, without dependence on the number of words in the listing. Similarly, we can define $W^{**}$ with elements $W^{**}_{ij} = W_{ij}/\sqrt{m_j}$, where $m_j = \sum_i W_{ij}$ counts the number of tokens of word type $j$ across all listings. This standardization down-weights the most common word types.  One can also combine these standardizations and use $\widetilde{W}_{ij} = W_{ij}/\sqrt{n_i\,m_j}$.  Alternatively, the popular method known as term frequency-inverse document frequency (TF-IDF)  emphasizes less common word types, but counts the number of documents in which a word appears rather than its frequency overall.  In the simplest version of TF-IDF,  let $d_j = \sum _i\one{W_{ij} >
 0}$ count the number of listings in which word type $j$ appears and replace $W_{ij}$ by
 $W_{ij} \log(n/d_j)$.  For example, if the word type ``.'' appears in
 every listing, then $\log n/d_j = 0$, removing this ubiquitous
 type from the construction of principal components. \citet{turney10} provides further motivation for these and other weightings used in LSA.  In our application, we use $\widetilde{W}$ because of  its ability to recover structure for data generated by the probability model described in Section \ref{sec:topic}.  In the expressions that follow, we generically use $W$ to stand for any of these weighted versions of the counts.
 
 
 We compute the leading principal components of $W$ by using random projections.  Random projections produce accurate approximations to the truncated SVD of $W$.  Though not absolutely necessary in this application -- one can compute the usual SVD of $W$ by regular means -- random projections become necessary as the number of documents and size of the vocabulary grow.  Denote the SVD of the document-word matrix $W$ by
 \begin{equation}
       W = U D V' = \sum _{i=1}^{\min(m,n)} \la_i U_i V_i' \;,
 \label{eq:W}
 \end{equation}
 where $U = [U_1,\ldots,U_n]$ and $V=[V_1,\ldots,V_m]$ are orthonormal, and $D = \mbox{diag}(\la_i)$ is a $n \times m$ diagonal matrix,
 with the singular values $\la_1 \ge \la_2 \ge \cdots \ge \la_{\min(m,n)}$ along the diagonal.  The columns of $U$ are the eigenvectors of $XX'$ (the sought principal components,
 or left singular vectors), and the columns of $V$ are
 the eigenvectors of $X'X$.  The truncated, or thin, SVD zeros all but the
 largest, say, $k$ singular values and produces an approximate
 factorization of $W$ that we denote
 \begin{equation}
       W_{1:k} \approx U_{1:k} D_k V_{1:k}' = \sum _{i=1}^k \la_i U_i V_i' \;,
 \label{eq:Wk}
 \end{equation}
 $U_{1:k}$ and $V_{1:k}$ hold the first $k$ columns of $U$ and $V$, respectively, and $D_k$ is the corresponding leading portion of $D$. We use the colon in a subscript to distinguish the matrix holding several columns from the columns themselves.  The computation of $U_{1:k}$ by direct means is relatively slow (a few hours of a current desktop computer), even for this problem with about 7,384 rows and 5,707 columns.  To speed this calculation, we exploit random projection algorithms defined and analyzed in \citet{tropp10}.  Random projections produce $U_{1:k}$ in less than a minute in our application and are essential in larger problems.

 
 Suppose in the ideal case that $W$ essentially has rank $k$ in the sense that $\la_k \gg 0$ and $\la_{k+1} \approx 0$.  Let $\ell$
 denote an integer slightly larger than $k$ \citep[see][for the
 details]{tropp10}.   Form an $m \times \ell$ matrix $\Omega_{\ell}$ with random
 elements $\Omega_{ij} \sim N(0,1)$, independently, and compute the product
 $A_{\ell} = (W W')^q W \Omega_{\ell}$ for small $q$.  (We set $q = 4$.)  The initial multiplication of $W$ by $\Omega_\ell$ reduces the number of columns
 from $m$ to $\ell$;  subsequent multiplication by $W W'$ improves the
 approximation, resembling the power method for finding eigenvalues. $(W W')^q W$ has the same singular vectors as $W$, but its singular values are $\la_j^{q+1}$, which typically increases the spread between $\la_k$ and $\la_{k+1}$.   The matrix product that defines $A_\ell$ can be done quickly by exploiting
 the sparsity of $W$.  Let $Q_{\ell}$ denote an orthonormal basis for the range of $A_{\ell}$, such as through a QR factorization $A_\ell = Q_\ell R_\ell$.  At the end of these steps, \citet{tropp10} shows that we obtain a low-rank factorization of $W$ in the sense that 
 \begin{equation}
   \norm{W - Q_{\ell} Q_\ell' W} \le  \left(1+11 \sqrt{\ell} \min(m,n)\right) \la_{k+1}
\label{tropp}
 \end{equation}
 with high probability.  $\norm{\cdot}$ denotes the spectral norm of a matrix.
 To obtain the SVD of $W$, we need only compute the SVD of the much smaller matrix $C_\ell = Q_\ell'W$.  Write the SVD of $C_\ell$ as $C_\ell = U_c D_c V_c'$ with subscripts to distinguish this factorization from \eqn{W}.  If we plug this expression into the low-rank representation for $W$, we obtain
\begin{equation*}
     W \approx Q_{\ell} Q_\ell' W = Q_\ell C_\ell = Q_\ell U_c D_c V_c' \;.
\end{equation*}
The first $k$ columns of $Q_\ell U_c$ approximate  $U_{1:k}$. \citet{tropp10}
 provides a thorough analysis this algorithm, so we
 illustrate its performance within the context of the analysis of real estate listings.


 The spectrum of the context-word matrix $W$ lacks the distinct gap that would suggest the rank needed to obtain an accurate low-rank approximation.  Figure \ref{fig:spectrum} graphs the singular values of the several weighted versions of $W$ on a log-log scale. (Figure \ref{fig:spectrum} requires computing the exact SVD of ${W}$ to obtain the singular values.  Figure \ref{fig:spectratwo} described in Section \ref{sec:topic} shows similar spectra of simulated data.)  The singular values for each weighting fall off steadily, without a clear gap that would identify an appropriate rank.  The linear decay in the plot suggests a power law for the singular values, $\la_i \propto i^{-\eta}$.  The rate $\eta$ is larger for the spectra when $W$ is left as raw frequencies or with row standardization.  Either version with column standardization has a smaller exponent.  The smaller $\eta$ encountered with column standardization implies makes the use of power iterations in the random projection essential to identify the singular vectors.  In every case, however, the absence of a clear rank complicates the choice of how many components to use in a regression, so once again we use $AIC_c$ with the features ordered by singular value.
 
 
 \begin{figure}
 \caption{ 
 	\label{fig:spectrum}
	{\sl The singular values $\la_i$ of the document-term matrix $W$ decay relatively slowly, following a power law $\la_i \propto i^{-\eta}$.  } The rate of decay $\eta \approx 0.6$ for $W$ without standardization (solid line) or with row standardization (dot-dash). Column standardization (short dash) and both row and column standardization (long dash) produce a slower rate of decay, $\eta \approx 0.25$.}
 \centerline{
 \vspace{0.1in}
 \includegraphics[width=4in]{figures/spectrum} }
 \vspace{0.2in}
 \end{figure}
   

The conversion of words into LSA components produces a qualitative change in the 
structure of the $t$-statistics.  Figure \ref{fig:lsatstats} summarizes the $|t|$-statistics from a regression using the first 1,000 LSA 
 component features in the same format
 as Figure \ref{fig:wordtstats}).  As with words, the model includes a polynomial in the length of the listings. 
 Unlike the regression on frequencies of word types, the average magnitude of the absolute values of the $t$-statistics in this regression decline roughly monotonically, with the most significant effects present in the initial components and gradually falling off. Some signal remains in the smaller components (the slope in the half-normal plot is 1.9), but sorting based on singular value is far more useful than sorting on word frequency.  The fitted model obtains  $R^2 = 0.756$ with \prs = 0.664, compared to \prs = 0.580 for the previous regression on 1,089 words.  Consequently, with more signal in the earlier components, $AIC_c$ finds a more predictive model with fewer coefficients that needed for individual words.  The sequence of $AIC_c$ statistics for this model lies well below that for the regression on word frequencies in Figure \ref{fig:aic}.  $AICc$ selects the model that uses 728 LSA components, obtaining $R^2 = 0.734$ and $\prs  = 0.668$.  
 
\begin{figure}
\caption{  \label{fig:lsatstats}  
  {\sl Absolute $t$-statistics from the regression of log prices on the first 1,000 LSA components from $\widetilde{W}$ with row and column weighting.}  }
  \centerline{ \includegraphics[width=5in]{figures/lsa_tstats.pdf} }
\end{figure}



 
%--------------------------------------------------------------------------
\section{Token Space}
\label{sec:tokenspace}
%--------------------------------------------------------------------------

Token space refers to the vector space -- and the related ways of thinking -- generated by modeling the sequence of word tokens in a collection of text.  From this perspective, the frequencies in the document-word matrix $W$ become covariances between random variables produced by the observed sequence of tokens. This perspective is natural to statisticians and provides the basis for creating a wide range of features from text.  


Recall that we observe $n$ real-estate listings that we will now generically call `documents'.   Document $i$ has $n_i$ word tokens that have been chosen from a vocabulary of $m$ word types.  To distinguish word types from word tokens, we use $\omega$ to denote types and $w$ to denote words.  We model the tokens in document $i$ as observable random variables generated by a stochastic process $H(\theta_i)$.   Our choice of this notation signals that the documents share an underlying process $H$, but each document comes with its own parameters that influence that process.  Each observation of $H(\theta_i)$ is a sequence of word tokens, so $H(\theta_i)$ implies a probability distribution over all possible sequences $\{w_1,w_2, \ldots, w_{n_i}\}$ of word tokens with $w_i \in V$.  (Section \ref{sec:topic} gives an example of $H$.) The number of tokens $n_i$ is random.  We model documents as independent observations.  The word tokens of a document may be independent in some applications, but dependence is more likely\citep[\eg][]{fosterkakade07}.
Denote the list of all of the observed tokens, concatenated one after another, by
\begin{equation*}
   \ell = \underbrace{w_{11},\,w_{12},\ldots,w_{1n_1}}_{\mbox{doc 1}},\,
            \underbrace{w_{21},\ldots,w_{2n_2}}_{\mbox{doc 2}}, \,
            \ldots, \underbrace{w_{n1},\ldots,w_{nn_n}}_{\mbox{doc }n},
\end{equation*}
with total length $N = \sum n_i$.  An analogous common statistical model  describes longitudinal data in which, for example, we observe a varying number of measurements per subject.  The subjects are usually modeled as independent, but the repeated measurements are not.  These measurements are often numerical, such as a sequence of blood pressures.  


To obtain numerical data when modeling text, we use a large collection of indicator variables convert the text $\ell$ into a sparse matrix.  From $\ell$, define the $N \times m$  matrix $X$ in which $X_{ij} = 1$ if the $i$th word token in $\ell$ is of type $j$:
\begin{equation}
   X_{ij} = \left\{ \begin{array}{cc}
                   1 & \mbox{ if } \ell_i = w_j \cr
                   0 & \mbox{otherwise.}
                \end{array} \right.
\end{equation}
This conversion produces a large,  sparse matrix with a single 1 in each row:
\begin{equation}
  X =  \left( \rule{0em}{8em} \right.
  \begin{array}{cccccccc}
            \mbox{\scriptsize $\omega_1$} &
            \mbox{\scriptsize $\omega_2$} &
            \mbox{\scriptsize $\omega_3$} &
            \mbox{\scriptsize $\omega_4$} &
            \mbox{\scriptsize $\omega_5$} &
            \mbox{\scriptsize $\omega_6$} &   \cdots &
            \mbox{\scriptsize $\omega_m$}  \cr
            0  & 0 & 1 & 0 & 0 & 0 & \cdots & 0 \cr
            0  & 0  & 0 & 0 & 1 & 0 & \cdots & 0 \cr
             &&&  \vdots &&&&                                 \cr
            0  & 1  & 0 & 0 & 0 & 0 & \cdots & 0 \cr \hline
            0  & 0 & 0 & 0 & 1 & 0 & \cdots & 0 \cr
              &&&  \vdots &&&&                                 \cr
            1  & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \cr \hline
              &&&  \vdots &&&\ddots&                                 \cr
              \\
           \end{array}
        \left. \rule{0em}{8em} \right)
          \begin{array}{c}
         \cr \mbox{\scriptsize{doc 1}} \cr \cr \cr \cr \mbox{\scriptsize{doc 2}} \cr \cr \vdots
  \end{array}
        \;.
    \label{eq:X}
\end{equation}
This matrix is equivalent to the original text.  This numerical representation, however, puts us back in familiar territory in which we handle numbers rather than text. 


This conversion produces more than numbers.  It also changes the way we interpret computations such as the word frequencies $W$ used in LSA.  To obtain these counts, define an $N \times n$ context matrix $L$  that identifies the documents,
\begin{equation}
  L =  \left(  
           \begin{array}{ccccc}
            1  & 0 & 0 & \cdots & 0 \cr
            1  & 0 & 0 & \cdots & 0 \cr
             &&  \vdots &&                       \cr
            1 & 0 &  0 & \cdots & 0 \cr \hline
            0 & 1 & 0  & \cdots & 0 \cr
              &&  \vdots &&                        \cr
            0 & 1& 0 & \cdots & 0 \cr \hline
              &&  \vdots  &\ddots&             \cr
            0 & 0& 0 & \cdots & 1 \cr \hline        
           \end{array}
         \right) = I_n \otimes \one{n_i}  \;,
\end{equation}
where $\one{n}$ denotes a column vector of $n$ 1s. (This definition abuses the Kronecker product by allowing the right-hand vectors to have unequal length.)  Because we model the document length $n_i$ as the random outcome of the underlying process $H(\theta_i)$, $L$ also denotes an observed collection of random variables.  It is more compact to record these lengths as integers, but in token space, we model the random characteristics as vectors in $\R^{N}$.  The document-word matrix $W$ is then $N-1$ times the sample covariance matrix between the columns of $X$ and $L$,
\begin{equation*}
   W = (N-1) \cov(L,X) = L'X  \;.
\end{equation*}


\subsection{ CCA } % ------------------------------------------

Once we think of $W$ as a covariance matrix, it becomes natural to associate LSA with canonical correlation analysis (CCA) rather than PCA.  In order for the SVD of $W$ to produce canonical vectors, however, we have to standardize the columns of $L$ and $X$.  Let 
\begin{equation}
  S_L = L'L/(N-1) = \mbox{diag}(n_i) \quad \mbox{ and  } \quad S_X = X'X/(N-1)
\label{eq:SL}
\end{equation}
denote the uncentered covariance matrices given by the columns of $L$ and $X$, respectively.  The SVD of $S_L^{-1/2} W S_X^{-1/2}$ then yields the canonical correlations and vectors.  Standardizing $L$ is easy because its columns are orthogonal (when uncentered), but those of $X$ are not.  Commonly, then, to avoid computing the $m \times m$ matrix $X'X$, one uses the approximation $\widetilde{S}_X = \mbox{diag}(m_j)$, with $m_j$ counting the number of tokens of word type $j$.   We then complete the approximate CCA analysis by finding the SVD of  $\widetilde{W} = S_L^{-1/2} W \widetilde{S}_X^{-1/2}$.   


The numerical representation \eqn{X} also supports a variety of other common featurizations.  The sequential nature of the rows of $X$ suggest ideas from time series analysis, particularly lags.  For instance, we might find interesting patterns in the association between adjacent word types in positions $i-1$ and $i$.  Let $X_{-1}$ denote the lag of $X$ obtained by inserting a leading row of zeros and removing the final row.  Rather than use the document matrix $L$ to define the context of a word, this featurization uses the prior word.  The counts that produce the covariance between $X$ and $X_{-1}$ define the bigram matrix $B$,
 \begin{equation}
   B = X_{-1}' X   \;.
\end{equation}
Using the prior word to define a context removes the role of documents and treats the list $\ell$ as a stochastic process.  Whereas $W$ uses the wide context defined by the extent of a document, $B$ uses the narrow, very local context defined by the prior word.  It is also worth noticing another difference.  $W$ ignores word placement (sequencing) within a document, treating a document as a bag of words.  In contrast $B$ combines the documents and relies only on the sequence of word tokens. The wide context afforded by a document hints that $W$ emphasizes semantic similarity, whereas the narrow context of adjacency suggests $B$ emphasizes syntax.  Either approach can be effective.

 
As in LSA, we obtain features from the leading singular vectors of $B$.  Again, random projection speeds this calculation.  Let $\widetilde{U}_{B,k}$ denote the leading $k$ singular vectors obtained from the approximate SVD of the standardized bigram matrix $S_X^{-1/2} B S_X^{-1/2}$.  The singular vectors in $\widetilde{U}_{B,k}$ are sometimes called ``eigenwords'' because of the way in which they form directions in word space \ras{cite lyle or someone using this name}.  Each row of $\widetilde{U}_{B,k}$ represents a word type as a point in $\R^k$.   To build features for regression, we simply locate each document at the average position of its words, $diag(1/n_i)\,L'\widetilde{U}_{B,k}$.
Figure \ref{fig:bigramtstats} shows the $|t|$-statistics from regressing real estate prices on the leading 1,000 document centroids computed from the bigram matrix.  These effects resemble the diffuse signal obtained using raw words (Figure \ref{fig:wordtstats}) and lack the concentration of signal into the leading components found in the LSA components (Figure \ref{fig:lsatstats}).  It should not be surprising, then, to find that bigram centroids perform similarly to words as predictors of price (Figure \ref{fig:aic}).  The minimum $AIC_c$ occurs when using 1,110 bigram centroids, for which \prs = 0.600.  This is higher than reached by simply using the word frequencies (0.580), but inferior to the fit produced by fewer LSA components (0.622).

 
\begin{figure}
\caption{  \label{fig:bigramtstats}  
  {\sl Absolute $t$-statistics from the regression of log prices on the first 1,000 bigram centroids.}  }
  \centerline{ \includegraphics[width=5in]{figures/bigram_tstats.pdf} }
\end{figure}


%--------------------------------------------------------------------------
\section{Probability models}
\label{sec:topic}
%--------------------------------------------------------------------------

Topic models have become common as a generative model to explain and rationalize the use of a nonparametric Bayesian methods for text.  Within this context, a topic is a probability distribution over a vocabulary of words.  Topic models for text propose that the observed text of a document was produced by sampling words according to the mixture of distributions produced by the topics in this document. Topic models treat words as exchangeable and model the text of a document as a multiset as in LSA.

To provide some explanation for the evident success of this direct approach to building regressors from text, we offer a hypothetical data generating process and study the implications of this DGP for regression modeling.  The DGP is essentially that used in topic modeling \citep{blei12}.  In machine learning, topic modeling is an unsupervised technique typically used to cluster documents based on the presence of latent ``topics'' revealed by a hierarchical Bayesian  model.  Our method of featurizing text is also unsupervised,  but we seek to predict an explicit response rather than uncover latent clusters.  Nonetheless, we can study how our procedure would perform were the data generated by such a model.
 
We developed the following model to reproduce the following stylized facts from the real estate listings.  The items include reference to the figure that shows this property of the listings when available.    Namely, data produced by the model should reproduce these characteristics of the observed data:
\begin{enumerate}
 \item Log normal distribution for the prices, or in our generating process, an normally distributed response (Figure \ref{fig:prices}).
  \item There should be a moderate correlation between the length of the document and the response.
  (Figure \ref{fig:parsed}).
 \item The frequencies of the word types in the vocabulary should approximate a Zipf distribution (Figure \ref{fig:zipf}).
  \item Predictive signal should concentrate in the leading LSA components, but be diffuse when modeled using words (Figure \ref{fig:lsatstats}).
  \item Document lengths should have a skewed distribution.
\end{enumerate}

Although topic models are a natural means to motivate vector space models, standard topic models do not capture the connection between document length and the response.  In a topic model, the length of a document does not reveal the number of topics that are present in the document.  This feature is clearly present in real estate listingsl the authors of listings write more about valuable properties than cheaper properties.  Descriptions of more expensive properties, those with numerous attractive attributes are longer.  Also, a description only mentions an attribute, such as a fireplace, when the attribute is present.   An ad would not mention that a property did not have a fireplace.  The  model developed in the following section captures this length effect as well as the other stylized facts observed in the listings.


\subsection{ An Alternative Topic Model } % -------------------------

The overall perspective of our DGP is that of a topic model: each property described by a real-estate listing possesses a set of attributes that collectively influence its listed price.   Examples of attributes are a fireplace or various types of hardwood floors.  We describe the model in the context of real-estate listings, but the methods are general.
 
To begin, this model links the presence of a set of unobserved attributes to the price of a property.  Let ${\cal A} = \{a_1,\ldots, a_K\}$ denote the set of  all possible attributes and assign value $\beta_k$ to attribute $a_k$.  The set ${\cal L}_i \subset {\cal A}$ denotes the attributes of the $i$th listing.  The attributes for each property are then obtained by sampling attributes from $\cal A$ (without replacement) until the sum of assigned values exceeds $\mu_i$, the target mean for the response.  In this sense, the set ${\cal L}_i$ is budget constrained. The owner of each property is limited by a budget amount $\mu_i$ in choosing to purchase attributes for her property.  The budget amounts in this model are
\begin{equation}
  \mu_i  \sim N(\tilde\mu, \tilde\sigma^2)
\label{eq:mui}
\end{equation}
in order to produce a distribution of shape similar to that of the observed log prices. If we let $\pi(1), \ldots, \pi(K)$ denote a random permutation of attributes, then the number of attributes $k_i$ obtained with budget $\mu_i$ is 
\begin{equation}
	k_i = \min \{ \kappa:  \mu \le \sum_{k=1}^{\kappa} \beta_{\pi(k)} \} - b_i \;,
	\mbox{ with }
	{\cal L}_i = \{a_{\pi(1)}, \ldots, a_{\pi(k_i)} \} \;,
\label{eq:ki}
\end{equation}
where $b_i$ is a random 0/1 Bernoulli r.v.  The addition of $b_i$ to the index $\kappa$ implies that the sum of attribute values is equally likely to be larger or smaller than $\mu$.  For convenience,  collect the indicators that identify which attributes appear in which properties into the $n \times K$ binary matrix $A$,
\begin{equation}
  \underset{n \times K}{A} = \left[ A_{ij} =
              \left\{ \begin{array}{cc} 1 & \mbox{ if } a_j \in {\cal L}_i \cr 0 & \mbox{otherwise.} 
                       \end{array} \right. \right]  \;.
\end{equation}
The $i$th row of $A$ gives the attributes in the $i$th document; for example, $\sum_j A_{ij} = |{\cal L}_i|$. The matrix $A$ is not observed in practice; the whole point of featurizing text is to recover some sense of this matrix from text. The expected price of a property is the sum of the values of its attributes,
\begin{equation}
	y_i = \sum_k A_{ik} \beta_k + \sigma \ep_i, \quad \ep_i \sim N(0,1) \;.
\label{eq:yi}
\end{equation}
The amount of noise, reflecting other unmeasured small effects, added  in \eqn{y} controls the fit of the idealized regression of $y$ on the latent attributes $A$.  


The collection of attributes also determines the text of the associated listing.  As in a familiar topic model, each attribute defines a multinomial distribution over the vocabulary of words, and so we identify the distribution for attribute $k$ by a point on the $m$-dimensional simplex.  The words in a listing are then sampled according to the mixture of distributions determined by its attributes.  


The distribution for each attribute has two components.  All attributes share a Zipf distribution over the entire vocabulary.  The attributes are distinguished by a second, attribute specific distribution.  Divide the vocabulary of $m$ words into $m_c$ common words shared by topics and $m - m_c$ words that are randomly sampled for each topic.  The distribution for an attribute convolves a Zipf distribution over the vocabulary with a topic-specific multinomial distribution over the $m - m_c$ attribute-specific words. Let 
\begin{equation*}
     p^z_i = \frac{1}{c\;i}, \qquad c = \sum_{i=1}^{m} 1/i 
\end{equation*}
denote a vector of probabilities that decay as in a Zipf distribution with exponent -1. The Zipf component $p^z$ is common to attributes and hence not useful to distinguish the attributes in a document. The second component of the distribution over words for an attribute is randomly generated by  sampling from a spiked Dirichlet distribution over words $m_c+1, \ldots,m$  
 \begin{equation}
     p^s_{m_c+1:m,k} \sim  \mbox{Dir}(\alpha,m-m_c)  \;, \quad \mbox{ with } p^s_{ik} = 0, i = 1,\ldots,m_c.
\label{eq:ps}
\end{equation}
Because we set $m_c = m/10$ in our simulations,  starting the support of the attribute-specific distribution at $m_c+1$ implies that this distribution does not put mass on words given large probability by $p^z$. This second distribution is specific to each attribute and allows one to recognize an attribute from the text.  We choose a small value of the parameter $0 < \alpha$ of the Dirichlet to produce a rather spiked distribution.   To appreciate how $\al$ affects the Dirichlet distribution, consider how to simulate a sample of size $m$ from a Dirichlet distribution with parameter $\al$.  First, generate $x_1,\ldots,x_n \sim \mbox{Gamma}(\al)$ and compute the sum $S = \sum_i x_i$.  Then $x_1/S,\ldots,x_m/S \sim \mbox{Dir}(\al,m)$.  A gamma distribution for small $\al$ is very skewed and so the resulting probabilities concentrate on a few coordinates. The distribution over the full vocabulary mixes the probability vectors over common and specific words, 
\begin{equation}
  P_k = (q_z \; p_k^z, (1-q_z) \; p_k^s)  \quad \mbox{ where } \quad 0 \le q_z \le 1.
  \label{eq:Pk}
\end{equation}


The expected word frequencies for a document are proportional to sums of selected rows of $P$.  A scaling parameter  $\la$ determines the average number of words per attribute.   Arrange the distributions for the attributes into a $K \times m$ matrix $P$, putting $P_k$ into row $k$.   Let $W$ denote the $n \times m$ matrix with the frequencies of word types (the document/word matrix), simulated as
\begin{equation}
	W \sim \mbox{Poisson} \left(\la \, A \, P \right) \;.
  \label{eq:simW}
\end{equation}
In this expression, $\mbox{Poisson}(M)$ denotes a matrix of independent Poisson random variables with means determined by the corresponding elements of the like-sized matrix $M$.

 
 Given that the document/word matrix $W$ has been  generated by the process defined in \eqn{simW}, the success of latent semantic analysis is particularly easy to see. Intuitively, LSA is well-matched to this DGP because both treat a document as a bag-of-words.  Let $D_m$ denote an $n \times n$ diagonal matrix with the  word counts $m_i$ along the diagonal.  Then the expected value of the document/word matrix $W$ is the sum of $K$ outer products:
\begin{equation}
    \ev W = \la \sum_k A_k \, P_k' \;.
  \label{eq:EW}
\end{equation}
The matrix product factors as an outer product, just as an SVD represents a matrix. That is, if we write $X = UDV'$, then we can express the matrix product as the sum $X = \sum_j d_{j} u_j v_j'$ where $u_j$ and $v_j$ are the columns of $U$ and $V$, respectively.  For our models of text, the left singular vectors $U_W$ from \eqn{Wk} are related to the allocation of attributes over documents defined by $A$.  Of course, there are many ways to factor a matrix, and it is not apparent why the factorization provided by the SVD would produce good estimates of this factorization.  To obtain a good fit with regression, however, does not require that we either find $P$ or recover $A$.  Referring back to the model \eqn{y}, we need only recover the range of $A$.  And that is precisely what the random projection SVD produces.

\vspace{0.5in}

{\bf Does this bigram part remain? }


The connection of this DGP to bigrams is less obvious and relies more on stronger assumptions.  Bigrams count the frequency of the adjacent word types, a property we associate with the sequence of words rather than co-occurrence within a document.  To see how the analysis of bigrams can nonetheless produce useful regressors, we need to add either stronger assumptions to our DGP or incorporate some type of sequential dependence.  For example, we might assume that words associated with attributes appear in short phrases.   As in the bag-of-words model, words within a phrase are drawn independently from the distribution defined by a trait, but the generating process samples within an attribute for some length of time before transitioning to another attribute (as in an HMM).  If these phrases are relatively long, then we can approximate the expected counts in the bigram matrix as a weighted outer product of the probability vectors for the attributes.  We can obtain the same heuristic in our DGP by assuming that the probability distributions $P_k$ for the attributes  have (nearly) singular support on words.  That is, most words are associated with a unique trait (implying $P_{k_1}'P_{k_2}  \approx 0$).  In either case, the marginal probability of finding adjacent words types reveals the underlying probability distribution.  


For instance, suppose the traits have disjoint support on words and that documents have common length $n_i \equiv n_1$. Then the probability of finding  word types  $w_{m_1}$ and $w_{m_2}$ from trait $k$ adjacent to each other is 
 \begin{equation}
  P(w_{m_1},w_{m_2}) = \sum_i \left(\zeta_{ik}^2/n \right) P_{km_1} P_{km_2} 
                                     = h_k P_{km_1} P_{km_2}  \;.
  \label{eq:joint}
\end{equation}
Let $N = \sum m_i$ denote the total number of observed words. Using the expression \eqn{eq:joint}, the expected value of the bigram matrix factors as
 \begin{equation}
    \smfrac{1}{N} \ev B \approx \sum h_k P_k\; P_k' = P' H P \;, 
                     \qquad  H = \mbox{diag}(h_k)\;,
 \label{eq:B2}
 \end{equation}
Again, a constrained factorization that produced probability vectors would be more appropriate if one truly believed this model.  In an ideal world, the singular values of the SVD would capture the unobserved marginal probabilities  $h_k$.  Expression \eqn{B2} also suggests why the left and right singular vectors of $B$ should match, or at least define a common subspace.

 
 Of course, even in expectation, the factorization of the bigram $B$ will not
 match  \eqn{eq:B2}; the SVD  only recovers a basis for the range of $B$.  Thus, the 
 singular vectors will mix the probability vectors.  We can
 then think about $U_B = P'O$ for some orthonormal matrix $O$.  That is, ideally
 the singular vectors span the correct space, but in a different basis so that
 we observe (again, in expectation)
 \begin{displaymath}
   w_i'U_b \approx (m_i \zeta_i'P)(P'O) = m_i \zeta_i \mbox{diag}(P_k'P_k) O
 \end{displaymath}
 Hence, when computing the correlations between the observed counts $w_i$ and the
 singular vectors, the norm of the probability distributions cancel and we
 obtain a rotation of  $\zeta_{i*}$ vector.  The rotation is the same, however,
 for all rows, and consequently our collection of regressors spans the same
 space as the unrotated $\zeta$.



%--------------------------------------------------------------------------------
\subsection{Examples of Simulated Data}

In the following two examples, we simulate $n = 5,000$ documents with words chosen from a vocabulary of $m = 1500$ words.  The simulation generates the response and text of these documents by sampling $K = 50$ attributes.  The budget parameters in \eqn{eq:mii} are $\tilde\mu = 12.2$ and $\tilde\sigma = 0.8$.  The attribute mean effects $\beta_k$ are independent draws from a gamma distribution with shape parameter 2 and scale parameter 1/2.  On average, each document is associated with 16 attributes. After forming  the random sums \eqn{ki}, we add enough measurement noise in \eqn{yi} so that 80\% of the variance in the response is explained by a linear regression on the attribute matrix $A$.  After rescaling to match the standard deviation of log prices, the simulated density of the response is similar to that of prices, but more Gaussian in shape.  Figure \ref{fig:density} shows the kernel density of the log of real-estate prices along with several simulated distributions following this procedure.  For simulating the words, the Zipf share of the attribute distribution $P_k$ in both of the following sections is $q_z = 0.4$, and the expected number of words per attribute is $\la = 6$.  


\begin{figure}
\caption{  \label{fig:density}  
  {\sl The density function of a several simulated distribution (gray) matches the location and shape of the density of the log prices of Chicago real estate (black), but is more Gaussian in shape.}  }
  \centerline{ \includegraphics[width=4in]{figures/density.pdf} }
\end{figure}
 
 
 The following examples show an illustrative simulation under different circumstances.  The simulated results shown here were chosen to show properties that persist over replications of the simulation. 
 
 
 \subsubsection{Nearly Disjoint Topic Distributions} % -----------------
  
In this first example, we consider how well estimation methods work when documents are generated by attributes with nearly disjoint specific words.  All attributes share the common Zipf distribution in this context, but have little overlap in the choice of specific words.  To obtain this behavior, we set the Dirichlet parameter in \eqn{ps} to $\al = 0.01$.   The probability distributions $P_k$ are not very distinct.  Figure \ref{fig:P} plots the probability vectors of two of the attribute distributions.  The points along the diagonal are the shared support probabilities for common words that follows a Zipf distribution.


\begin{figure}
\caption{ \label{fig:P}
{ \sl  Two probability distributions over words simulated from a Dirichlet distribution with parameter $\al = 0.01$ (left) and $\al = 0.025$ (right) have similar levels of common word support.}}
 \centerline{
 \vspace{0.1in}
 \includegraphics[width=7.5in]{figures/P} }
 \end{figure}


The distribution of words produced by this model  reproduce features of distribution found in real estate listings.  The average document has about 80 words.  Figure \ref{fig:zipfcorr} plots the frequencies of all word types in the collection of simulated documents.  These frequencies resemble those in Figure \ref{fig:zipf}, but produce a slope (-0.6) that a is closer to zero than observed in the listings.  Figure \ref{fig:zipfcorr} also graphs the simulated response on the length of the simulated documents.  The squared correlation is 0.16 in this example.  The smooth trend in the plot highlights a slight degree on no linearity in the association, though as in Figure \ref{fig:parsed}, most of the curvature occurs in the extremes of the range.

\begin{figure}
\caption{ \label{fig:zipfcorr}
{ \sl Simulated documents produce a Zipf-like distribution over word frequencies (left) and have lengths that are correlated with the response.}}
 \centerline{
 \vspace{0.1in}
 \includegraphics[width=7.5in]{figures/zipfcorr} }
 \end{figure}

 A major objective of having a probability model that can reproduce the stylized facts observed in the real estate listings was to gain some insight as to the best normalization of the LSA and bigram matrices.  We focus here on the choices for normalizing the word counts prior to computing the thin SVD in \eqn{Wk}.   To explore this here, Figure \ref{fig:spectra} shows the spectra obtained from four standardizations defined by \eqn{SL}  of $W$: none (raw frequencies), rows $S_L^{-1/2}W$, columns $WS_X^{-1/2}$), and both as used in the quick approximation to CCA  $S_L^{-1/2}WS_X^{-1/2}$.  (The spectra are scaled to fit all four into one plot. The spectra of the row (column) standardized versions was multiplied by 10 (50), and the spectrum with both normalizations was multiplied by 500.) Evidently, the break that correctly detects $K=50$ attributes requires column standardization, but is barely detectible in this figure.  Figure \ref{fig:gap} zooms in on the singular values of the versions with column standardization.  A discontinuity occurs at $K=50$.  Row standardization alone does not detect the break, and the combination of both row and column standardization does not improve, and may slightly conceal, the gap.  This plot would suggest using either column or CCA standardization for determining the number of latest attributes.  As in Figure \ref{spectrum}, the presence of row standardization flattens the spectrum of $W$.  In this example, if we again approximate the singular values $\la_i \propto i^{-\eta}$, then the use of column standardization -- which reveals the gap -- reduces $\eta$ from about 0.3 to 0.1.
 
 
\begin{figure}
\caption{ \label{fig:spectra} 
{ \sl Spectra obtained for the document/word matrix $W$ with no standardization, column standardization, row standardization, and both row and column standardization with Dirichlet parameter $\al = 0.025$.}  Singular values were scaled by constant factors in order to show all four on the same plot.}
 \centerline{
 \vspace{0.1in}
 \includegraphics[width=6.0in]{figures/spectra} }
 \end{figure}


\begin{figure}
\caption{ \label{fig:gap} 
{ \sl Magnification of the spectra with column standardization ($+$) and both row and column standardization ($\times$) show a discontinuity at the number of attributes ($K=50$).}  Singular values were scaled by constant factors in order to show both on the same plot.}
 \centerline{
 \vspace{0.1in}
 \includegraphics[width=6.0in]{figures/gap} }
 \end{figure}

Our objective in modeling text is, however, not to determine the latent variables or probability distributions, but rather lies in building regression models for the response from text.  Although it  appears either column or CCA normalization are equally useful, CCA  standardization produces a much better set of regressors.  Figure \ref{ccawins} convinced us to use CCA standardization.  The figure contrasts the absolute values of the $t$-statistics of the estimated coefficients from regressions of the simulated response on document length and  the first 250 row-standardized singular vectors to the results obtained for the first 250 CCA-standardized singular vectors. In both models, document length is not statistically significant when included with these singular vectors.  Although the overall fit obtained by these two collections of features is very similar ($R^2 = 0.268$ and $0.267$), the significant estimates produced by the CCA standardization concentrate in the leading singular vectors rather than spread out many components.  


\begin{figure}
\caption{ \label{fig:ccawins} 
{ \sl Absolute $t$ statistics produced by column standardization of $W$ and CCA standardization of $W$.}
  Both models include document length, which is not significant.}
 \centerline{
 \vspace{0.1in}
 \includegraphics[width=6.0in]{figures/ccawins} }
 \end{figure}

 
 \subsubsection{Overlapping Topic Distributions} % -----------------
 
Increasing the Dirichlet parameter to $\al=0.025$ produces substantially more overlap between the support of the distributions $P_k$.  All distributions share the Zipf probabilities, but are nonetheless distinguishable with $\al=0.01$.  With $\al$ set 2.5 times higher, the distinction disappears.  Figure \ref{fig:spectratwo} shows the spectra obtained with the various standardizations of $W$.  None show the gap evident in Figure \ref{fig:spectra} for $\al=0.01$, and we again see that column standardization (with or without row standardization) flattens the spectrum.  


The absence of a gap draws attention to a feature of the spectra that is less apparent in Figure \ref{fig:spectra}.  In particular, any form of column standardization reduces the spread among the singular values.  Without column standardization (top row of plots), the singular values drop rapidly.  With column standardization, the singular values decay more gradually, roughly linearly with index on this log scale.  Though suggestive, singular vectors are more predictive with column standardization.

 
\begin{figure}
\caption{ \label{fig:spectratwo} 
{ \sl Spectra obtained for the document/word matrix $W$ with no standardization, column standardization, row standardization, and both row and column standardization with Dirichlet parameter $\al = 0.025$.}  Singular values were scaled by constant factors in order to show all four on the same plot.}
 \centerline{
 \vspace{0.1in}
 \includegraphics[width=6.0in]{figures/spectra2} }
 \end{figure}

Although the ability to detect the number of attributes is lost with the increase in $\al$, the singular vectors remain predictive.  With $\al=0.025$, $R^2$ falls only slightly to 0.222 in this typical example.
 
%--------------------------------------------------------------------------
\section{Variable Selection and Cross Validation}
\label{sec:cv}
%--------------------------------------------------------------------------

The previous examples routinely estimate regression models with hundreds of explanatory variables.  Though large by traditional standards, these models do not suffer from the problems associated with over fitting because we have not used the data to pick the model.  We simply fit a large collection of regressors.  Evaluating such models is thus the province of classical statistical tests and criteria such as adjusted r-squared.  

As evidence that these models are not over fit, we offer the following example.  Consider the LSA regression that uses 500 principal components of $W$.  Adjusting for the effects of estimating the 500 coefficients and the intercept anticipates the out-of-sample mean squared error of prediction to be $s_e^2 (1+(p+1)/n)$.  This simple approximation averages over the regression design, ignoring leverage effects.  Plugging in the unbiased estimate $s_e^2 = 0.5649$ gives $0.5649 (1+501/7384) = 0.603$.  Because the values of the regressors in the test data do not match those in the training data, this estimator is typically smaller than the observed MSE by an amount depending on variation in the regressors.  

For comparison, we performed repeated 10-fold transductive cross validation.  Transductive cross-validation presumes that the full collection of regressors is available for the training data, which in this case implies that we have all of the data available to perform the principal components analysis.  Only the values of the response are hidden in each fold of the cross-validation; the collection of regressors is fixed.  We repeated the 10-fold cross-validation 20 times, each time randomly partitioning the cases into 10 folds. The observed average squared error was slightly higher than anticipated at $0.614 \pm 0.007$, but basically agreed with the estimate from the fitted model. 


%--------------------------------------------------------------------------
\section{Lighthouse Variables and Interpretation}
\label{sec:light}
%--------------------------------------------------------------------------
 
 

 Our emphasis on predictive accuracy does not necessarily produce an
 interpretable model, and one can use other data to create such structure.  Our
 explanatory variable resemble those from principal components analysis and
 share their anonymity.  To provide more interpretable regressors, the presence
 of partial quantitative information in real estate listings (\eg, some listings
 include the number of square feet) provides what we call lighthouse variables
 that can be used to derive more interpretable variables.  In our sample, few
 listings (about 6\%) indicate the number of square feet.  With so much missing
 data, this manually derived predictor is not very useful as an explanatory
 variable in a regression.  This partially observed variable can then be used to
 define a weighted sum of the anonymous text-derived features, producing a
 regressor that is both complete (no missing cases) and interpretable.  One
 could similarly use features from a lexicon to provide more interpretable
 features.


Though regression models are seldom causal, one is often tempted to interpret
 properties of the solution within the domain of application.  Because the
 predictors computed from the decompositions in the prior section describe
 subspaces rather than some specific property of words, interpretation is
 essentially non-existent.


 To obtain features that are interpretable, we exploit the pre<sence of
 characteristics that are occasionally observable.  For example,
 most home listings do not include the number of bathrooms.  An
 explanatory variable obtained by parsing this count is missing for 74\% of the property listings.  We can use this partially observed variable, however, to construct a more interpretable variable from either the principal components of $W$ or the bigram variables.  


 Let $z \in \Rn$ denote the partially observed or perhaps noisy data
 that measures a substantively interesting characteristic of the
 observations.  For our example, $z$ is the partially observed count of the number of bathrooms.  Rather than use $z$ directly as a predictor of $y$, we
 can use it to form an interpretable blend of, for instance, $U_W$.   In
 particular, we simply regress $z$ on these columns, finding the
 linear combination of these basis vectors most correlated with the
 observed variable.  This variable, call it $\hat{z}$ then becomes
 another regressor.  Because such variables can be used to guide the
 construction of interpretable combinations of the bases $U_W$ and
 $C$, we call these lighthouse variables.  
 
 
 In our example, the correlation between the number of bathrooms and the price of the listing is 0.42 for listings that show this characteristic.  This correlation is much smaller (0.19) if we fill the missing values with the mean number of bathrooms (Figure  \ref{fig:parsed}).  If we form the projection $\hat{z}$ given by regressing the observed counts on the corresponding rows of $U_W$, this new regressor has correlation 0.29 with the log of price.



%--------------------------------------------------------------------------
\section{Summary and Next Steps}
\label{sec:disc}
%--------------------------------------------------------------------------
  
  Regression modeling always benefits from greater substantive insight, and the modeling of text is no exception.  An obvious approach to building regressors from text data relies on a
 substantive analysis of the text.  For example, sentiment analysis constructs a
 domain-specific lexicon of positive and negative words.  In the context of real
 estate, one might suspect  words such as ``modern'' and ``spacious''  to be associated with more expensive properties, whereas
 ``Fannie Mae'' and ``fixer-upper'' to signal properties with lower prices.  The
 development of such lexicons has been an active area of research in sentiment
 analysis over the past decade \citep{taboada11}.  The development of a lexicon
 require substantial knowledge of the context and the results are known to be
 domain specific.  Each new problem requires a new lexicon.  The lexicon for
 pricing homes would be quite different from the lexicon for diagnosing patient
 health.  Our approach is also domain specific, but requires little user input
 and so can be highly automated.


 Our analysis here shows that one can exploit well-known methods of multivariate analysis to create regressors from unstructured text.  Compared to iterative methods based on MCMC, the computations are essentially immediate.  Surprisingly, the resulting regressors are quite predictive in several examples we have explored.  For example, we used this same methodology to model ratings assigned to wines based on tasting notes.  The tasting notes themselves are typically shorter than these real estate listings (averaging about 42 tokens compared to 72 for the listings), but we have a larger collection of about 21,000.  Using the methods demonstrated here, a regression using 250 principal components of $W$ explains about 66\% of the variation in ratings, we a remarkably similar distribution of effect sizes as shown in Figure \ref{fig:wine}.  Similarly, regression on the 250 left and 250 right regressors constructed from the bigram matrix explains about 68\% of the variation.  We plan to explore other applications in the near future.
 
 
 \begin{figure}
 \caption{ \label{fig:wine} \sl Regression t-statistics from a model that predicts wine ratings using 250 principal components of $W$ based on 21,000 wine tasting notes.}
 
 \centerline{
   \includegraphics[width=4in]{figures/wine.pdf}
   }
  \end{figure} 
  
  
   The connection to topic models is an important aspect of these results.  Topic models define a DGP for which the regressors that we construct capture the underlying data-generating mechanism.  If one accepts topic models as a reasonable working model of the semantics of text, then it is no accident that regressors constructed from text are predictive.


 Our work here merely introduces these methods, and we hope that this introduction will encourage more statisticians to engage problems in  modeling text.  Our results here also suggest several directions for further research:

   \begin{description}
   
   \item[n-grams.]  Our example uses bigrams to capture word associations captured by adjacent placement.  Other reasonable choices define different measures of context,  such as trigrams (sequence of 3 words) or skipped bigrams (words separated by some count of tokens).  Some preliminary results show, for instance, that trigrams offer modest gains, albeit at a nontrivial increase in computation.
   
   \item[Transfer learning.]  Transfer learning refers to learning what can be extrapolated from one situation to another.  In our context, it would be of interest to learn how well models developed from data in June 2013 work when applied to data from later time periods or different locations.  It is evident that the models shown here would not perform so well applied to the language of a different market, such as in Miami or Los Angeles.  Not only do the characteristics of valuable properties change, but  local conventions for phrasing listings are also likely to be different.  Having a methodology for distinguishing idiosyncratic local features from those that generalize in time or space would be valuable. 
   
  \item[Alternative forms of tokenization.] Would be interesting to explore the use of stemming to reduce the number of word types and with a larger collection of documents, to explore annotation (that would distinguish words by their part of speech).  Further parsing, lexical analysis.  Some readers will be troubled by the simplicity of the  bag-of-words representation of a document.  Our methods understand neither the English language nor the rules of grammar and spelling.  They do not attempt to untangle  multiple uses of the same word.  Linguists have debated the ability of such  representations to reveal the meaning of language, and it is clear that the bag of words representation loses information.  Just imagine cooking from``bag of words'' recipe or following a ``bag of words'' driving directions.  Nonetheless, this very direct representation produces very useful explanatory variables within our application.  We leave open the opportunity to embellish this approach with more domain specific methods of parsing, such as adding part-of-speech tags and lexical information.
   
   \item[Use of unsupervised data.] Most words are used only once or twice, meaning that we lack enough data to identify their connection to the response or indeed to other words.  As a partial remedy, it may be possible to build regressors that represent such words from larger, more generic text such as the collection of n-grams collected by Google.  Using this supplemental unsupervised data requires solving the problem of transfer learning, at least to some degree, but opens the door to much more extensive examples. 
   
   \item[Variable selection.]  The distribution of effects (such as shown by the $|t|$ statistics of the text-derived regressors) are poorly matched to the so-called `nearly black' model commonly adopted in research on the theory of variable selection.  Rather than have most of the predictive power concentrated in a very small number of regressors, these regressors spread the power of many.  
   
   It would also be interesting to explore these models for nonlinearity. Variable selection is perhaps unnecessary for using the regressors derived from $U_W$ and $C$, but essential if one hopes to detect and incorporate nonlinearity. In particular, searching for nonlinearity -- such as interactions -- requires variable selection.  Even a very large corpus of documents looks small compared to the number of possible second-order interactions.  Finally, the ordered presentation of the $|t|$ statistics suggests an opportunity for variable selection derived from alpha investing \citep{fosterstine08}.  Alpha investing is a procedure for testing a sequence of hypotheses that benefits from {\it a priori} ordering of the tests.
   
   \end{description}

One can generalize LSA from words and documents to collections of other items,
 such as phonemes in speech or tones in music \cite[called latent semantic
 mappingin][]{bellegarda05}.



%--------------------------------------------------------------------------
\section*{Acknowledgement}
%--------------------------------------------------------------------------

The authors thank Vanessa Villatoro from Trulia's PR Team for allowing us to scrape the data from their web site.


%--------------------------------------------------------------------------
% References
%--------------------------------------------------------------------------

\bibliography{../../../references/stat,../../../references/TextPapers/text}
\bibliographystyle{../bst/ims}

\end{document} %==========================================================
