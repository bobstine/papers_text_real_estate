%-*- mode: LaTex; outline-regexp: "\\\\section\\|\\\\subsection";fill-column: 80; -*-
\documentclass[10pt]{article}
\usepackage[longnamesfirst]{natbib}
\usepackage[usenames]{color}
\usepackage{graphicx}  % Macintosh pdf files for figures
\usepackage{amssymb}   % Real number symbol {\Bbb R}
\usepackage{bbm}
\input{../../standard}

% --- margins
\usepackage{../sty/simplemargins}
\setleftmargin{1in}   % 1 inch is NSF legal minimum
\setrightmargin{1in}  % 1 inch is NSF legal minimum
\settopmargin{1in}    % 1 inch is NSF legal minimum
\setbottommargin{1in} % 1 inch is NSF legal minimum

% --- Paragraph split, indents
\setlength{\parskip}{0.00in}
\setlength{\parindent}{0in}

% --- Line spacing
\renewcommand{\baselinestretch}{1.5}

% --- page numbers
\pagestyle{empty}  % so no page numbers

% --- Hypthenation
\sloppy  % fewer hyphenated
\hyphenation{stan-dard}
\hyphenation{among}

% --- Customized commands, abbreviations
\newcommand{\TIT}{{\it  {\tiny Featurizing Text (DRAFT, \today)}}}

% --- Header
\pagestyle{myheadings}
\markright{\TIT}

% --- Title

\title{ Featurizing Text: Converting Text into Predictors for
            Regression Analysis }
\author{
        Dean P. Foster\footnote{Research supported by NSF grant 1106743} 
        \ \ Mark Liberman 
        \ \ Robert A. Stine\footnotemark[\value{footnote}]   \\
        Department of Statistics                             \\
        The Wharton School of the University of Pennsylvania \\
        Philadelphia, PA 19104-6340                          
}

\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle 
%------------------------------------------------------------------------
\vspace{-.5in}
\abstract{  

 Modern data streams routinely combine text with the familiar numerical data
 used in regression analysis.  For example, listings for real estate that show
 the price of a property typically include a verbal description.  Some
 descriptions include numerical data, such as the number of rooms or the size of
 the home.  Many others, however, only verbally describe the property, often
 using an idiosyncratic vernacular.  For modeling such data, we describe several
 methods that that convert such text into numerical features suitable for
 regression analysis.  The proposed featurizing techniques create regressors
 directly from text, requiring minimal user input.  The techniques range naive
 to subtle.  One can simply use raw counts of words, obtain principal components
 from these counts, or build regressors from counts of adjacent words.  Our
 example that models real estate prices illustrates the surprising success of
 these methods.  To partially explain this success, we offer a motivating
 probabilistic model.  Because the derived regressors are difficult to
 interpret, we further show how the presence of partial quantitative features
 extracted from text can elucidate the structure of a model.

}

%------------------------------------------------------------------------
\vspace{0.05in}

\noindent
{\it Key Phrases: sentiment analysis, n-gram, latent semantic analysis, text mining } 

\clearpage

{\bf To Do: }
Are the CCA results in the dual space?
\\

% ----------------------------------------------------------------------
\section{New outline}
% ----------------------------------------------------------------------


\begin{description}
\item{Introduction} \\ Intro to problem, we're taking direct approach.
  Introduce application and do regression based on looking for regular
 expressions.  Now see how to do better.  Novelty lies in use of random
 projections, simplicity.
\item{ The application } \\ 

 Tokenization.  Zipf distribution.  Word-document matrix. 

\item{LSA regression} \\ 

 The obvious regression, with development of several ways to weight the
 elements, recommending for this application the sqrt n weighting and CCA
 weights.  Emphasize same as PC regression.  One can go further with quadratic
 effects (pairs of words), though to what gain in short documents (too sparse)?
  Centroid connection useful if doing transcendent cross validation, so the
 averaging/centroid connection is useful here.

\item{Bigrams and context matrix} \\ 
 
 Revisit term-document matrix as word-context matrix.  Develop the statistical
 sense of CCA between token/type and token/document.  then do that perspective
 of the bigram matrix.  Issue: Those CCA weights don't seem to help.

\item{Relate to topic models} \\ 
 Simulation

\item{Variable selection}  \\
 AIC and connection to CV analysis.  Plot of reflections.

\item{Discussion}  \\
Lighthouse variables combine regular expression variables and linguistic variables. Other applications to wine.

\end{description}

 Theme of co-occurrence within a window of varying length, document or
adjancency.  Alternative weights such as TF-IDF rather than entropy.

 Software packages, such as (very elaborate, comprehensive) those in R
 \citep[tm][]{feinerer08} or Stanford NLP or openNLP.

 stopwords... ``and'', ``for'', ``in'', ``is'', ``it'', ``not'', ``the'', ``to'')
 
\ras{Would sparse CCA (Witten et al) be useful. \citet{witten09}}
 
 \clearpage


% ----------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}
% ----------------------------------------------------------------------

 Modern data streams routinely combine text with numerical data suitable for
 regression analysis.  For example, patient medical records combine lab
 measurements with physician comments.  Analogously, online product ratings
 shown at Amazon blend explicit characteristics with verbal commentary.  How can
 one easily incorporate the information in such text into a regression model
 that predicts patient outcomes or product ratings?  Our objective here is to
 illustrate several particularly straightforward techniques for featurizing
 text, turning text into numerical variables suitable for regression modeling.
 
 
 As a specific example, we build a regression model to predict advertised prices
 of real estate properties from their listings.  Our data describe $n=$7,384
 property listings for Chicago, IL, extracted (with permission) from trulia.com
 on June 12, 2013.  At the time, trulia.com showed 30,322 listings for Chicago,
 but most were foreclosures that we excluded from our analysis.  The response in
 our models is the log of the listed price.  The distribution of the listed
 prices is right skewed, so we transformed the response to a log scale as shown
 in Figure \ref{fig:prices}.  The log transformation produces a roughly Gaussian
 distribution, with a hint of fat tails for produced by inexpensive properties
 with prices near \$25,000.  (This display uses the base 10 log of the prices
 for easy interpretation; subsequent models use natural logs.)
 
 
 \begin{figure}
 \caption{ \label{fig:prices} { \sl The distribution of prices for real estate
 in Chicago is highly skewed.  A log transformation produces data that are
 nearly normal.}  }
 \centerline{
 \vspace{0.1in}
 \includegraphics[width=5in]{figures/prices} }
 \vspace{0.2in}
 \end{figure}


 The listings that describe these properties are verbal, written in an
 idiosyncratic vernacular familiar only to those who are house hunting.  Some
 listings contain quantitative characteristics, but many do not.  The four
 following listings are typical examples of the data.  The initial value is the
 listed price.

 \begin{verbatim}
    $399000 Stunning skyline views like something from a postcard are yours
    with this large 2 bed, 2 bath loft in Dearborn Tower!  Detailed
    hrdwd floors throughout the unit compliment an open kitchen and
    spacious living-room and dining-room /w walk-in closet, steam
    shower and marble entry.  Parking available. 

    $13000 4 bedroom, 2 bath 2 story frame home. Property features a
    large kitchen, living-room and a full basement. This is a Fannie Mae
    Homepath property. 

    $65000 Great short sale opportunity...  Brick 2 flat with 3 bdrm
    each unit. 4 or more cars parking. Easy to show. 

    $29900 This 3 flat with all 3 bed units is truly a great
    investment!! This property also comes with a full attic that has
    the potential of a build-out-thats a possible 4 unit building in a 
    great area!!  Blocks from lake and transportation. Looking for a
    deal in todays market - here is the one!!! 
 \end{verbatim}

 \noindent
 Listings do not obey the grammatical rules of English.  Some authors write in
 sentences, others not, and a variety of abbreviations appear.  The style of
 punctuation varies from spartan to effusive (particularly exclamation marks),
 and the length of the listing runs from several words to a long paragraph.  The
 average listing has 73 words, and the distribution of the lengths is right
 skewed.  The shortest listing has 2 words, whereas the longest has 568 words.
  This variation in the lengths of the descriptions suggests that modeling the
 prices from this text will require some form of weighting: we simply do not
 know so much about properties with short descriptions.

  
 A natural approach to the problem for a statistician is to construct
 intuitively appealing regressors from the listings.  For example, one might
 conjecture that an agent has a lot more to say about an expensive property than
 one in need of repair, implying that the length of the listing for a property
 may be predictive of its price.  In a different vein, one can use regular
 expressions -- pattern matching -- to extract specific numerical data from the
 listings.  For example, the first listing indicates that this property has two
 bedrooms and two bathrooms.  A regular expression can be designed to extract
 numbers that precede the word ``bath'' from the listings.  Constructing such
 regular expressions, however, is a labor-intensive process that must be done on
 a case-by-case basis.  The patterns become complex because they must allow
 common abbreviations, such as ``bth'', ``bath'' and ``bthrm'' in order to match
 counts of the number of bathrooms.  Complexity aside, the greater problem with
 explicit pattern matching is that most listings omit these characteristics.
  The only numerical data common to every listing is the response, the
 advertised price.  For these listings, our regular expressions found that 6\%
 of the listings indicate the number of square feet, 26\% indicate the number of
 bathrooms, and 42\% give the number of bedrooms.  More complex regular
 expressions would likely find more matches, but the gains are likely to be few.
  As illustrated below, one also faces a Type I/Type II trade-off.  Simple
 regular expressions miss some matches, but more aggressive expressions match
 inadvertently.


 The four scatterplots in Figure \ref{fig:parsed} summarize the marginal
 association between the log of prices and these constructed predictors,
 including the number of words in descriptions.  For listings with no match, we
 filled the missing values with the averages of the observed cases.  These
 appear as columns of gray points located at the mean of the variable on the $x$
 axis in Figure \ref{fig:parsed}.  The correlations between these variables and
 the log of the prices vary from moderate to nearly zero.  The length of the
 listing has the largest correlation ($r=0.40$). This frame includes the fit of
 a fifth-order polynomial that recovers the evident nonlinearity of this
 association.  The nonlinearity is highly significant, but increases the amount
 of explained variation from 0.16 to only 0.20.  The improvement in fit is
 slight because most of the data lie within the linear range of the fit.
  Missing data has a large impact on the correlations with the other extracted
 features shown in Figure \ref{fig:parsed}.  The correlations for complete cases
 are 0.42 for the number of bathrooms, 0.26 for the log of the number of square
 feet, and 0.09 for the number of bedrooms.  If missing data are included, the
 correlations become much smaller (the second correlation shown in each frame).
  These plots also show several anomalies.  For example, the scatterplot of the
 log price on log length shows a cluster of 25 listings, all with exactly 265
 words (log 265=5.6).  These are not errors: All of these different properties
 were described in a common format by a government agency.  The scatterplot of
 the log of prices on the log of the square footage also shows a clear outlier;
 this outlier is a consequence of aggressive matching in a regular expression.
  A typo in a description (``has 1sfam'') led our regular expressions to find a
 property with 1 square foot.

 \begin{figure}
  \caption{ \label{fig:parsed} { \sl Extracted characteristics have moderate to
 slight positive association with the log of the prices. } Gray points in the
 figures identify cases that omit the explanatory variable; the first shown
 correlation in each frame uses only the observed data; the following smaller 
 value includes mean-filled listings. }
 
\centerline{
 \vspace{0.1in}
 \includegraphics[width=5in]{figures/parsed} }
 \vspace{0.2in}
 \end{figure}


 Table \ref{tab:parsed} summarizes the fit of a regression of the log of price
 on predictors built from the four extracted features. The model includes a
 fifth degree polynomial in the log of the length of the description and missing
 data indicators.  These indicators are coded as 1 when the associated regular
 expression found a match and are coded as 0 otherwise.  Aside from the
 components of the polynomial, the features are not highly collinear, and the
 multiple regression echoes the marginal associations observed in Figure
 \ref{fig:parsed}.  Features related to the lengths of listings explain the most
 variation, followed by the number of bathrooms.  Unlike the marginal
 associations, the log of the number of square feet and its missing indicator
 are highly significant.  The missing indicators for bedrooms and bathrooms are
 not.  The model obtains adjusted R-squared $\ol{R}^2 = 0.232$, with residual
 standard deviation $s_e =1.06$.

 
 \begin{table}[ht]
\caption{ \label{tab:parsed} {\sl OLS multiple regression of log prices on the
 parsed explanatory variables and  indicators of observed values.}}
\centering
\begin{tabular}{lrrrr}
  \hline
    Feature       & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
  Intercept       & 7.9985 & 0.4749 & 16.84 & 0.0000 \\ 
  log Tokens      & 39.0183 & 1.0945 & 35.65 & 0.0000 \\ 
  $(\mbox{log Tokens})^2$   & 1.0086 & 1.0642 & 0.95 & 0.3433 \\ 
  $(\mbox{log Tokens})^3$ & -18.2556 & 1.0612 & -17.20 & 0.0000 \\ 
  $(\mbox{log Tokens})^4$ & -4.8956 & 1.0594 & -4.62 & 0.0000 \\ 
  $(\mbox{log Tokens})^5$ & 7.9094 & 1.0584 & 7.47 & 0.0000 \\ 
  log Sq Feet     & 0.4039 & 0.0579 & 6.98 & 0.0000 \\ 
  Sq Feet obs     & 0.6254 & 0.0594 & 10.53 & 0.0000 \\ 
  Bedrooms        & 0.0080 & 0.0165 & 0.48 & 0.6299 \\ 
  Bedrooms obs    & -0.0084 & 0.0299 & -0.28 & 0.7783 \\ 
  Bathrooms       & 0.4015 & 0.0300 & 13.37 & 0.0000 \\ 
  Bathrooms obs   & -0.0199 & 0.0336 & -0.59 & 0.5537 \\ 
   \hline
   \multicolumn{5}{c}{$\ol{R}^2 = 0.232, \; s_e = 1.06$}\\
   \hline
\end{tabular}
\end{table}

 
 
 Had listings been composed in a consistent manner with a standardized set of
 characteristics, such constructive modeling would surely be more successful.
  Our goal here takes us in a different direction that, in contrast to this
 constructive approach, explores automatic methods that thrive in this
 unformatted, irregular context.  These methods are collectively known as {\em
 vector space models} in computational linguistics \citep[e.g.][]{turney10}.  A
 vector space model (VSM) characterizes each document as a point in some
 $p$-dimensional vector space (think \Rp).  VSMs were originated in artificial
 intelligence and were adopted for text applications to construct a system for
 document retrieval known as latent semantic indexing \citep{deerwester88}.
  Latent semantic indexing clusters documents to enable a query to locate
 related items.  VSMs are closely related to techniques that statisticians will
 find familiar: principal components analysis (PCA) and canonical correlation
 analysis (CCA).  The underlying computational engine is a singular value
 decomposition (SVD), aided by random projections.  (The use of the SVD
 occasionally leads to VSMs being described as ``spectral algorithms'' which
 should not be confused with frequency domain methods for time series.)  There
 has been considerable debate as to how VSMs represent language; we leave to
 linguistics the task of explaining why such simple representations of text
 might capture deeper meaning \citep{deerwester90, landauer97, bullinaria07,
 turney10}.


 VSMs are commonly found in unsupervised applications such as clustering, but
 our application here finds that they are equally effective in supervised
 models.  In essence, each document becomes a point in \Rp, and these
 coordinates become regressors.  These regressors can be used alone or in
 combination with traditional variables, such as those obtained from a lexicon,
 regular expression, or semantic model.  The example of real estate listings
 illustrates the impact of various choices on the predictive accuracy.  For
 example, a regression using the automated features produced by a simple VSM
 explains over two-thirds of the variation in listed prices for real estate in
 Chicago.  The addition of several substantively derived variables adds little.
  Though we do not emphasize its use here, variable selection can be employed to
 reduce the ensemble of regressors without sacrificing predictive accuracy.  A
 similar application that models personality traits derived from Facebook
 messages appears in \citet{ungar13}.


 The remainder of this paper develops as follows.  The next section reviews
 latent semantic analysis (LSA) and uses this well-known method from
 computational linguistics to featurize text.  This method corresponds to
 pricipal components regression.  Section \ref{sec:cca} presents vector space
 models such as LSA from a novel statistical perspective that emphasizes the
 analysis of ``observations,'' samples drawn from a population.  LSA is seen to
 correspond to a CCA between indicator variables for specific words and
 indicator variables for the context of those words.  Other definitions of the
 context of a word allow one to construct other types of features.  Though
 embarrassingly simple to describe, it is more subtle to appreciate why such
 vector space methods work.  Section \ref{sec:topicmodels} offers one
 explanation by relating vector space models to the structure of generative
 topic models.  Section \ref{sec:cv} returns to regression models for real
 estate with a discussion of the use of variable selection methods ands use
 cross-validation to measure the success of methods and to compare several
 models.  Variable selection is particularly relevant if one chooses to search
 for nonlinear behavior.  We close in Section \ref{sec:disc} with a discussion
 and collection of future projects.



%--------------------------------------------------------------------------
\section{Featurizing with Latent Semantic Analysis}
\label{sec:lsa}
%--------------------------------------------------------------------------

 Latent semantic analysis (LSA) is essentially a principle components analysis
 of a matrix of counts of how often each word appears in each document (in our
 case, the frequency of words in a property listing).  Using these components as
 features in regression then amounts to a principle components regression.
  Quite a few details, however, need to be resolved in any application. For
 example, what is a word?  This section describes the relevant choices within
 the context of building a regression model for real estate prices.
 
 \subsection{ Tokenization }  % ------------------

 LSA begins by converting the source text into {\em word tokens}.  A word token
 is a sequence of one or more characters that represents an instance of a unique
 {\em word type}.  This conversion of text into tokens is known as tokenization.
  Tokenization requires making numerous, often subtle, choices.  For example, is
 the number ``735'' a word token?  Do the tokens ``state'' and ``State''
 represent the same type?  Are ``room'' and ``rooms'' instances of the same word
 type?  To answer these questions, we adopt a standard, simple approach: we
 convert all text to lower case, distinguish punctuation characters as separate
 types, and replace instances of rare word types by an invariant token.  To
 illustrate some of the issues in tokenization, the following string appears in
 a listing:
 \begin{verbatim}
   Brick flat, 2 bdrm.  With two-car garage. \end{verbatim} 
 \noindent
 Separated into tokens, this text becomes a list of 10 tokens representing 9
 word types (angle brackets surround punctuation tokens)
 \begin{verbatim}
   {brick, flat, <,>, 2, bdrm, <.>, with, two-car, garage,<.> } \end{verbatim} 
 \noindent
 We leave embedded hyphens in place and do not correct spelling errors and
 typos.  Abbreviations remain as given.  References such as the texts by
 \citet{manning99} and \citet{jurafsky09} describe more elaborate choices, such
 as stemming and annotation, that can be incorporated into
 tokenization. \citet{turney10} give a concise overview.  Notice that
 tokenization only considers the letters that identify type type, not how the
 word appears.  Consequently word types are not distinguished by meaning or use
 (does not distinguish homographs).

  
 When applied to our data from Chicago, the 7,384 property listings contain
 536,485 word tokens representing 15,227 unique word types.  As usual in text,
 the most common word types occur frequently whereas most words appear
 infrequently.  More than half of these tokens appear only once or twice,
 providing little exposure to how the word is used.  We clustered these rare
 types into one category ($<$OOV$>$, for ``out of vocabulary''), resulting in a
 reduced vocabulary of $m = 5,707$ word types.  The most common word types are
 punctuation: `.'  occurs 40,227 times and `,' occurs 33,746 times.  Following
 these come the collection of seldom seen word types (OOV, 11,478), `and'
 (frequency 11,032), `!' (7,894) and `in' (7,418).  Figure \ref{fig:zipf} graphs
 the log of the frequency of these word types versus the log of their ranks.  It
 is common in text to find a linear trend with slope near 1 in this graph, the
 signature of a Zipf distribution \citep{zipf35, baayen02}.  In that case, the
 rank of a word type times its frequency would be constant.  Even though the
 text of real estate listings is not standard English, one expects to find
 counts resembling those produced by a power law \citep{clauset09}.  The shown
 line (with log-log slope -0.927) holds only for more common word types.  For
 less common words (here, outside the most common 500 words), the frequencies
 drop off more rapidly.  (This change in slope is also seen for words in
 Wikipedia.)


 \begin{figure}
 \caption{ \label{fig:zipf} { \sl The log of the frequency of word types in the
 listings for properties in Chicago is roughly linear in the log of the rank of
 the words.}  The shown line $\log \mbox{freq} = 10.9 - 0.93 \log \mbox{rank}$ 
 was fit to the most common words with ranks 1 through 500.  }

 \centerline{
 \vspace{0.1in}
 \includegraphics[width=3.5in]{figures/zipf} }
 \vspace{0.2in}
 \end{figure}


 Once the source text has been tokenized, we collect counts of how often each
 type appears within a listing.  These counts treat each listing as a ``bag of
 words,'' a multiset that does not distinguish the placement of words within a
 listing. Scrambling the words within each listing would produce the same
 summary.  Let $n$ denote the number of listings; each listing is an observation
 in the usual sense of statistics.  Let $V$ denote the vocabulary, the set of
 $m$ distinct word types.  The $n \times m$ matrix $W$ holds the counts of these
 word types across the listings; $w_{ij}$ is the number of times word type $j$
 appears in the $i$th document.  (All vectors in our notation are column
 vectors.)  (The transpose of $W$, called the term-document matrix, is common
 within computational linguistics. We follow the convention within statistics
 and arrange independent observations -- the listings -- as rows.)  The matrix
 $W$ is sparse: most documents use a small portion of the vocabulary.


\subsection{ Regressing on Word Types }  % ----------------------

 Before building models using singular vectors, we set a baseline by directly
 regressing the log prices response on the word counts in $W$.  Table
 \ref{tab:regrInd} summarizes the fit using the most common 2,000 words, along
 with the lengths $m_i$ and log lengths.  We include these in every model to
 avoid including features constructed from text that simply capture the length
 effect evident in Figure \ref{fig:parsed}.  Overall, this model produces
 $\ol{R}^2 = 0.681$.  Residual plots show fat tails (consistent with that in the
 prices, Figure \ref{fig:prices}) but no clear evidence of heteroscedasticity
 that might be anticipated due to the differences in the document lengths.  The
 most significant word is ``vacant'' with $t=-8.5$; not surprisingly, the
 addition of this word to a listing indicates a lower average price.  In
 contrast, the presence of additional out-of-vocabulary words (denoted ``OOV'')
 suggest with higher than average prices ($t=6.3$). Figure \ref{fig:regrInd}
 summarizes the distribution of all t-statistics in this model.  Twenty-two
 words are not used due to singularities among these counts, leaving 1,978
 estimated coefficients.  The dashed line in the scatterplot of $|t_j|$ on $j$
 in the left panel of Figure \ref{fig:regrInd} is the Bonferroni threshold
 $\Phi^{-1}(1-0.025/1978) \approx 4.21$.  Only 14 estimates exceed this
 threshold for statistical significance.  The nearly flat red line in the figure
 is a lowess smooth of the $|t_j|$.  The half-normal plot in the right panel of
 Figure \ref{fig:regrInd} confirms the diffuse signal in this model: the
 distribution of the fitted $t$-statistics is not far from the null
 distribution.  The gray line in the half-normal plot is the diagonal; the red
 line is the fitted regression of the smallest 200 $|t|$-statistics on the
 corresponding quantiles.  For a sample from a standard normal distribution, the
 slope of a line fit to the $t$ values should be 1.  The slope of the fitted
 line for these estimates is significantly larger, but clearly the signal is
 widely spread over these estimates.  A regression on counts for 14 words whose
 estimates exceed the Bonferroni bound (Table \ref{tab:regrInd}) obtains
 $\ol{R}^2 = 0.191$.  Thus, this estimation problem differs from the so-called
 ``nearly black'' models often studied in research on variable selection.  In
 those models, a few estimates stand out from a noisy background. In this
 application, much of the signal lies embedded in that background.

 \begin{table}
 \caption{ \label{tab:regrInd} {\sl Multiple regression of log prices on 
  counts from the document/word matrix $W$ for the most common 2,000 words.}  The table shows the 14 estimates that exceed the Bonferroni threshold for statistical significance.} 

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & $t$ & Pr($>|t|$) \\ 
  \hline
  vacant & -0.5518 & 0.0652 & -8.46 & 0.0000 \\ 
  deed & -1.3155 & 0.1557 & -8.45 & 0.0000 \\ 
  OOV & 0.0373 & 0.0059 & 6.33 & 0.0000 \\ 
  units & 0.1929 & 0.0342 & 5.64 & 0.0000 \\ 
  discount & -1.4959 & 0.2992 & -5.00 & 0.0000 \\ 
  investment & -0.2334 & 0.0497 & -4.70 & 0.0000 \\ 
  most & 0.3350 & 0.0736 & 4.55 & 0.0000 \\ 
  bucktown & 0.3570 & 0.0790 & 4.52 & 0.0000 \\ 
  sf & 0.3305 & 0.0741 & 4.46 & 0.0000 \\ 
  pullman & -0.6244 & 0.1423 & -4.39 & 0.0000 \\ 
  bedroom & -0.0978 & 0.0227 & -4.31 & 0.0000 \\ 
  terraces & 0.7074 & 0.1650 & 4.29 & 0.0000 \\ 
  fenced & -0.2466 & 0.0578 & -4.27 & 0.0000 \\ 
  scb & -5.5914 & 1.3244 & -4.22 & 0.0000 \\ 
 \hline
\end{tabular}
 
    $s_e =  0.682$ with $R^2 =  0.766$, $\ol{R}^2 =  0.681$  
\end{center}
\end{table}



 \begin{figure}
  \caption{ \label{fig:regrInd} { \sl Distribution of the t-statistics for the regression of log prices on counts of the most frequent 2,000 word types (columns of $W$).}}
 \centerline{  \includegraphics[width=6in]{figures/tstatRegrInd}    }
 \vspace{0.2in}
 \end{figure}


Rather than arbitrarily regress log prices on 2,000 words, we can use variable selection to decide the number of words to use.  Rather than pick a subset of words, we use the corrected AIC statistic ($AICc$) to determine the size of the model.  The initial model includes the number of tokens in each document, $m$.  By initializing the model with this count, common words are not added simply because they are common but rather because the use of the word is associated with the price.  Otherwise, common tokens such as a period or comma might appear predictive  because they occur frequently in longer documents.  Figure \ref{fig:cumr2} shows the accumulated $R^2$ statistic, with the words added in two orders: in order of decreasing (solid black) or increasing (dashed black) frequency.  Adding the most frequent words first provides a better fit with fewer variables.  The gray curve in the figure shows the contribution to $R^2$ of the words in order of frequency, but with the magnitude of the change in $R^2$ as if added in the other order.  The difference between the black and gray curves shows the influence of collinearity among the word types.  For example, when added as the fourth variable, an OOV token boosts $R^2$ from 0.1417 to 0.1608.  If added four places from the last variable, these tokens add only 0.0003 to  $R^2$.  When constructed in order of decreasing frequency, 219 of these 3,000 word types were redundant and not added to the model.


\begin{figure}
\caption{  \label{fig:cumr2}  
  {\sl Accumulated $R^2$ statistics of regression models with words added in order of decreasing (solid black) or increasing (dashed black) frequency.} The gray curve shows the contributions having adjusted for subsequent words and shows the amount of collinearity among the word counts.}  
  \centerline{ \includegraphics[width=4in]{figures/cum_r2.pdf} }
\end{figure}

Although the $R^2$ statistic grows steadily with increasing model size, the slowing rate of growth suggests that we can find a more predictive model with fewer coefficients.  To this end, we use the corrected AIC statistic \citep{hurvich89}
\begin{equation}
    AIC_{c}(k) = n \log \frac{RSS(k)}{n} + \frac{n+k}{1-(k+2)/n} \;,
\end{equation}
where $k$ is the number of estimated parameters in the regression.  Figure \ref{fig:aicwords} plots $AIC_c$ for models constructed from counts of word times in order of decreasing (solid) and increasing (dashed) frequency.  Again we find that building up a model from the most common words produces a better fit with fewer terms.  The best fit in this example occurs with $k=1,090$ words (4 frequency, but collinear words were omitted).


\begin{figure}
\caption{  \label{fig:aicwords}  
  {\sl Corrected AIC statistics for regression models that add words in the order of decreasing (solid) or increasing (dashed) frequency.}  The vertical gray segment shows the minimum of $AIC_c$.}
  \centerline{ \includegraphics[width=4in]{figures/aic_words.pdf} }
\end{figure}


It is useful to notice the importance of using a model selection criterion such as $AIC_c$ in this context rather than a classical test.  For example, consider the classical $F$ test between the  model that minimizes $AIC_c$ and a larger model derived from the most frequent 2,000 words.  The $F$-test indicates that the model derived from 2,000 words is ``better.''  The  partial-$F$ test of the variation added by including the additional words is statistically significant ($F = 1.93$ with $p$-value far less than 0.001).  The larger model explains statistically significantly more variation in log prices.  That said, because the larger model also has many more estimated coefficients, it is less predictive.  Repeated 10-fold cross validation gives mean squared prediction error 0.714 for the larger model, compared to 0.639 for the smaller model identified by AICc.



   



 \subsection{ PCA  via Random Projection }  % ------------------

LSA proceeds by computing the principal components of $W$ or a weighted version of $W$.   We consider the effects of three ways to weight the elements of $W$:
\begin{description}
  \item[ {\bf Standardized} ] \ Divide each row of $W$ by the square root of the
 number of tokens $n_i = \sum_j W_{ij}$ within each listing, computing $W^s_{ij}
 = W_{ij}/\sqrt{n_i}$.  If the tokens within a listing were a random sample from
 a multinomial distribution across the word types, then $\Var(W_{ij}) \propto
 n_i$.  Dividing $W_{ij}$ by $\sqrt{n_i}$ equalizes the variation across
 listings.
   \item[ {\bf CCA}] (Canonical correlation analysis) \ $W$ is related to a
 canonical correlation analysis in a sense explained further in the following
 section.  Essentially this weighing combines the standardization of the prior
 weighting by also weighting the columns of $W$ (word types) before constructing
 the principal components.  We denote the resulting matrix ???
  \item [{\bf TF-IDF}] (Term frequency-inverse document frequency) \ This
 weighting gives more emphasis to the appearance of less common word types. We
 use the simplest variation on this approach.  Let $d_j = \sum _i\one{W_{ij} >
 0}$ count the number of listings in which word type $j$ appears and compute
 $W^{T}_{ij} = W_{ij} \log(n/d_j)$.  For example, if the type ``.'' appears in
 every listing, then $d_j = n$ and $W^{T}_{ij} = 0$, removing this ubiquitous
 type from the construction of the principal components.
\end{description}
 \citet{turney10} summarizes the motivation for various weightings used in LSA.
  Curiously, we find that it makes little difference when predicting prices
 which weights are used.  In the expressions that follow, we generically use $W$
 to stand for any of these weighted versions of the counts.
 
 
 We compute the principal components of $W$ by using random projections that
 estimate the truncated SVD of $W$.  The SVD of the count matrix $W$ produces
 the decomposition
 \begin{equation}
       W = U D V'
 \end{equation}
 where $U$ and $V$ are orthonormal, and $D$ is a $n \times m$ diagonal matrix,
 with the singular values of $X$ along the diagonal.  The columns the $n \times
 n$ matrix $U$ are the eigenvectors of $X'X$ (the sought principal components,
 or left singular vectors), and the columns of the $m \times m$ matrix $V$ are
 the eigenvectors of $XX'$.  The truncated, or thin, SVD zeros all but the
 largest, say, $k \ll m$ singular values and produces an approximate
 factorization of $W$, which we denote
 \begin{equation}
       W \approx U_k D_k V_k'
 \label{eq:Wk}
 \end{equation}
 Each matrix on the right-hand side of \eqn{Wk} consists of the first $k$
 columns of the full decomposition.  Because of the size of $W$, the computation
 of $W_k$ can be very slow.  Since we only need $U_k$, however, we can easily
 exploit the algorithms based on random projection \citep{tropp10}. Let $k'$
 denote an integer slightly larger than $k$ \citep[see][for the
 details]{tropp10}.  Then form a $m \times k'$ matrix $\Omega_{k'}$ with random
 elements $\Omega_{ij} \sim N(0,1)$, independently, and compute the product
 $P_{k'} = W \,Omega$.  This multiplication can be done quickly by exploiting
 the sparsity of $W$.  Since $k' \ll m$, we can either compute the SVD
 \begin{equation}
   P_{k'} = \tilde{U}_{k'} \tilde{D}_{k'} \tilde{V}_{k'}  \;,
 \end{equation}
 or more simply find an orthogonal basis for its columns (such as by a QR
 decomposition).  Though remarkably simple, the leading $k$ columns of
 $\tilde{U}_k$ accurately approximate the range of $U_k$.  \citet{tropp10}
 provides a thorough analysis of the accuracy of this approximation, so we
 illustrate its performance within the context of the real estate listings.


 Random projection accurately captures the range of a matrix, but requires a
 smaller SVD in order to reproduce the singular vectors.  Figure
 \ref{fig:approx} shows canonical correlations between the exact and approximate
 SVD of $W$ produced by random projection.  Figure \ref{fig:approx} shows the
 canonical correlations between the exact singular vectors in $U_{1:k}$ and the
 approximation
 \begin{equation}
      A_k = W W' (W \Omega_k)  \;.
 \label{eq:approx1}
 \end{equation}
 The initial multiplication of $W$ by $\Omega_k$ reduces the number of columns
 from $m$ to $k$; the subsequent multiplication by $W W'$ improves the
 approximation, resembling the power method for finding eigenvalues.  The
 leading canonical correlations between $U_{k}$ and $A_k$ are close to 1, and
 then drop off from about $0.75 k$ onward.  This tendency holds for small $k =
 100$ and larger choices.  The random projection captures the stonger components
 of the range of $U_k$, but much of the energy associated with weaker components
 lies outside the range of $A_k$.   Hence, recovering the singular vectors
 requires one to form the SVD of $A_k$.  If $k \ll m$, this smaller
 factorization is a much faster than forming the full decomposition. 

 \begin{figure}
 \caption{ \label{fig:approx} { \sl Canonical correlations show the accuracy of
 the approximation of the SVD of $W$ by singular value decompositions.}  }

 \centerline{
 \vspace{0.1in}
 \includegraphics[width=3.5in]{figures/approx} }
 \vspace{0.2in}
 \end{figure}


 Figure \ref{fig:spectrum} shows the first 500 singular values of $W$.  After
 the first few elements, the singular values fall off relatively slowly.  

  For instance, the 5 leading canonical correlations between $U_{100}$ and
 $A_{100}$ exceed 0.99999.  The correlations between the first right-singular
 vector of $W$ (the first column of $U$) with the first 5 columns of $A_k$,
 however, are -0.905, 0.396, 0.131, -0.091, and 0.154.  
 
 \begin{figure}
 \caption{ 
 	\label{fig:spectrum}
	{\sl The spectrum of the document-term matrix $W$ decays relatively slowly 
	      after the initial sharp drop. }}

 \centerline{
 \vspace{0.1in}
 \includegraphics[width=3.5in]{figures/spectrum} }
 \vspace{0.2in}
 \end{figure}
   


 
%--------------------------------------------------------------------------
\section{Featurizing with Latent Semantic Analysis (LSA)}
\label{sec:cca}
%--------------------------------------------------------------------------



 The second matrix narrows the context and relies entirely upon ordering.  The bigram matrix $B$ counts how often words  appear adjacent to each other, narrowing the context from a document to a pair of words.  The document/word and bigram matrices thus represent two extremes of a common approach:  Associate words that co-occur within some context.  $W$ uses the wide window provided by a document, whereas $B$ uses the narrowest window. The wide context afforded by a document hints that $W$ emphasizes semantic similarity, whereas the narrow window of adjacency that defines $B$ places more emphasis on local syntax.  Curiously, we find either approach effective and use both.


 The bigram matrix counts how often word types occur adjacent to each other.
  Let $B$ define the $M \times M$ matrix produced from the sequence of tokens
 for all documents combined (the corpus).  $B_{ij}$ counts how often word-type
 $i$ precedes work-type $j$ within the corpus.  Whereas $W$ ignores word
 placement (sequencing) within a document, $B$ combines counts over all documents and relies on the sequence of word tokens.

 
 A second application of the SVD produces regressors from the bigram matrix $B$.
  Let
 \begin{equation}
       B = \tilde{U}_B \tilde{D}_B \tilde{V}_B'
 \label{eq:svdB}
 \end{equation}
 define the SVD of $B$, and again use matrices without $\sim$ to denote components of the truncated SVD:  $U_B$ and $V_B$ denote the first $k_B$ columns of $\tilde{U}_B$ and $\tilde{V}_B$, respectively.  As in the
 decomposition of $W$, the number of singular vector to retain is a user-defined choice.  We generally keep $k_W = k_B$.  Because $B$ is $M
 \times M$, these singular vectors define points in $\R^M$ and are sometimes referred to as ``eigenwords'' because of the way in which they form directions in word space \ras{cite}.  The $i$th row of $U_B$  locates the word type $w_i$ within $\R^{k_B}$  (all of the following applies  to $V_B$ as well).  To build regressors, we locate each document at a point within this same space.  We can think of this location in two, nearly equivalent ways that emphasize either the rows or columns of $U_B$.  The two methods differ in a sum is normalized.  The row-oriented motivation is particularly simple: a document is positioned at the average of the positions of its words.  For example, the $i$th document is located at $w_i'U_B/m_i$.  Alternatively, emphasizing columns, we can compute the correlation between the columns of $U_B$ with the bag-of-words representations of the documents.  Because the columns of $U_B$  and $V_B$ are orthonormal, these correlations are given by
 \begin{equation}
     C = [C_l C_r] 
         = \mbox{diag}(\norm{w_i}^{-1})W[U_B \; V_B]   \;, 
            \quad \mbox{ where } \quad  \normsq{x} = \sum x_i^2  \;.
 \label{eq:C}
 \end{equation}
 The $i$th row of of the $n \times 2k_B$ matrix $C$ is the vector of correlations between the bag-of-words representation $w_i$ and the singular vectors of $B$.  In our models for real-estate listings, the columns of $C$ form the second bundle of regressors.


 It is worthwhile to take note of two properties of these calculations that are
 important in practice.  First, one needs to take advantage of sparsity in the
 matrices $W$ and $B$ to reduce memory usage and to increase the speed of
 computing matrix products.  Second, the computation of the SVD of a large
 matrix can be quite slow.  For example, computing the SVD of $B$ is of order
 $O(M^3)$ and one can easily have vocabulary of $M=$10,000 or more word types.
  To speed this calculation, we exploit random projection algorithms defined and
 analyzed in \citet{tropp10}.


%--------------------------------------------------------------------------
\section{Predicting Prices of Real Estate}
%--------------------------------------------------------------------------


 The next model uses regressors created from the SVD of the document/word
 count matrix $W$ defined in equation \eqn{svdW}.  We retained $k_W = 100,\, 200, \, \ldots, 500$ singular vectors of $W$.   Our analysis suggests each of these collections of singular vectors both retains too many insignificant features while at the same time omits others that are predictive.  Broadly speaking, the leading singular vectors (those with larger singular values) are more predictive than subsequent singular vectors.  That said, not all of the leading vectors are statistically significant nor do all of the later singular vectors have coefficients near zero.   As a result, variable selection from a yet larger collection of singular vectors may provide a better fit, a task we defer to Section \ref{sec:cv}.  
 

Each collection of singular vectors explains more variation than the simple model derived from several parsed words, but none find all of the signal captured by the larger collection of 2,000 word indicators.  A regression of log prices on the 100 leading singular vectors attains $\ol{R}^2 = 0.49$, more than twice that of the model with parsed variables.  Adding more singular vectors produces statistically significant, though diminishing improvements.  The collection of 500 singular vectors of $W$ produces $\ol{R}^2 = 0.61$, which is less than the $\ol{R}^2 = 0.68$ derived from individual word counts.  Table \ref{tab:regrW}  shows several of the estimated coefficients and summarizes the overall fits of these models.  As often seen in principal components regression, the statistical significance of the singular vectors is not monotonic in the order of the singular vectors. That said, the leading singular vectors tend to be more relevant than those that follow. Figure \ref{fig:svdregr} summarizes the significance of the estimated coefficients in the same fashion as Figure \ref{fig:regrInd} with a plot of the absolute $t$-statistics and a half-normal plot. Most of the main leading singular vectors are significant, with an increasing proportion of insignificant variables as the position in the decomposition increases.  In comparison to the significance for the coefficients of the word indicators, these singular vectors by-and-large are more consistently predictive with less noise.  The slope of the fit in the half-normal plot (using the least significant 200 estimates) is 2.9.  Residual analysis again finds fat tails with only a hint of heteroscedasticity.
 
 
 Not only can one continue to add further singular vectors, one can also consider nonlinearities in the form of interactions among these singular vectors.  The addition of interactions improves the fit of this model immensely by taking advantage of nonlinearities (\ie, synergies among the eigenword structure).  For example, a regression using just the first 20 singular vectors of $W$ obtains $\ol{R}^2 = 0.31$.  Adding interactions among these (an additional 190 explanatory variables since no powers are added) improves the fit significantly to $\ol{R}^2 = 0.41$.  Fitting models with interactions drawn from a larger collection of features more generally requires some form of selection or regularization.  We pursue this further in Section \ref{sec:cv}.
 

 \begin{table}
 \caption{ \label{tab:regrW} {\sl Multiple regression of log prices on
  singular vectors of the document/word count matrix $W$.}  The first table shows estimated coefficients for first 10 and last 3 singular vectors with $k_w = 500$.  }

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
  D1 & -62.0847 & 2.4891 & -24.94 & 0.0000 \\ 
  D2 & -7.6494 & 0.9373 & -8.16 & 0.0000 \\ 
  D3 & -12.5468 & 0.7558 & -16.60 & 0.0000 \\ 
  D4 & 10.4948 & 0.8262 & 12.70 & 0.0000 \\ 
  D5 & -0.8683 & 0.7645 & -1.14 & 0.2561 \\ 
  D6 & 7.6848 & 0.8036 & 9.56 & 0.0000 \\ 
  D7 & -17.8753 & 0.7547 & -23.69 & 0.0000 \\ 
  D8 & 18.8346 & 0.7577 & 24.86 & 0.0000 \\ 
  D9 & 3.9515 & 0.7540 & 5.24 & 0.0000 \\ 
  D10 & 4.0974 & 0.7521 & 5.45 & 0.0000 \\  
  $\vdots$ & & & & \\
  D498 & 1.2327 & 0.7516 & 1.64 & 0.1010 \\ 
  D499 & 1.3265 & 0.7516 & 1.76 & 0.0776 \\ 
  D500 & 2.5296 & 0.7516 & 3.37 & 0.0008 \\ 
   \hline
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{ccccc}
	$k_w$   & Residual SD & $F$ & $R^2$  & $\ol{R}^2$  \cr
	100       &  0.863   & 72.1  & 0.496  & 0.489  \cr
	200       &  0.810   & 45.9  & 0.561  & 0.549  \cr
	300       &  0.778   & 35.5  & 0.601  & 0.584  \cr
	400       &  0.765   & 28.5  & 0.620  & 0.598  \cr
	500       &  0.752   & 24.3  & 0.638  & 0.612
\end{tabular}
\end{center}
\end{table}


 The third regression uses features derived from the SVD of the bigram matrix
 $B$ defined in \eqn{svdB}.  For this analysis, we retained $k_B=100, 200, \ldots, 500$ left and right singular vectors ($k_B$ columns in each of $U_B$ and $V_B$) and for each computed the associated matrix of correlations $C$.  With $2 \, k_B = 1,000$ left and right singular vectors, the largest $\ol{R}^2 = 0.66$, slightly more than the 0.61 obtained using the 500 singular vectors defined from $W$.  Table \ref{tab:regrB} summarizes these fits.  Using correlations from either the left or right singular vectors alone (derived from either $U_B$ or $V_B$) explains significantly less variation; for example, a regression on 500 correlation vectors derived from  $U_B$ alone  produces $\ol{R}^2 = 0.61$ (about the same as obtained from the 500 singular vectors of $W$). The use of the left and right singular vectors produces collinearity among the explanatory variables.  A consequence of this collinearity is a large number of insignificant regressors in the fitted model.  Figure \ref{fig:svdregr}(b) shows the p-values generated by the 500 singular vectors of correlations.  Compared to the singular vectors derived from $W$ shown in Figure \ref{fig:svdregr}(a), these regressors have more diffuse signal.  The slope in the half-normal plot (again, derived from the least significant 200 estimates) is 2 (compared to 2.9 for the regressors derived from $W$).  Also, the trend in the half-normal plot is concave rather than convex; the most significant variables are less significant than anticipated by the signal in the smaller estimates.

 \begin{figure}
 \caption{ \label{fig:svdregr} { \sl T-statistics of the singular value regressors
 for (a) the singular vectors of $W$ and (b) the left and right singular vectors
 of $B$.}}
 \vspace{0.1in}
 \centerline{  \includegraphics[width=6in]{figures/regrW}  }
 \centerline{  \includegraphics[width=6in]{figures/regrB}  }
 \end{figure}


\begin{table}
\caption{ \label{tab:regrB} {\sl Multiple regression of log prices on regressors derived from the bigram matrix $B$.}  Each regression uses correlations derived from $k_B$ left and $k_B$ right singular vectors.  }
\begin{center}
\begin{tabular}{ccccc}
	$2\,k_B$   & Residual SD &  $F$ & $R^2$  & $\ol{R}^2$  \cr
	200           &  0.842           &  40.0 &  0.527 &  0.514 \cr
	400           &  0.779           &  26.8 &  0.605 &  0.583 \cr
	600           &  0.750           &  20.6 &  0.645 &  0.614 \cr
	800           &  0.724           &  17.4 &  0.679 &  0.640 \cr
	1000         &  0.704           &  15.3 &  0.705 &  0.660
\end{tabular}
\end{center}
\end{table}


We can concentrate more of the regression signal into the leading components by using canonical correlation analysis.  Figure \ref{fig:cca} shows the canonical correlations between  $C_l$ and $C_r$.  The correlations remain close to 1 for about the first 100 or so canonical variables.  Rather than use the columns of $C_l$ as regressors, we can use the canonical variables from this analysis.  Figure \ref{fig:regrBcca} summarizes the estimates using the 500 predictors.  The regression signal is now much more concentrated in the leading canonical variables, resembling the structure found by LSA (Figure \ref{fig:regrW}).  The overall fit, of course, matches that obtained by using $C_l$ since the canonical vectors are linear transformations of $C_l$.  We conjecture that CCA has this effect because, under the simple topic model introduced in Section \ref{sec:model} that follows, the left and right singular vectors of $B$ measure essentially the same thing and aligning these via CCA produces a better measure of that common structure.  This alignment, however, removes the distinction due to word order that the bigram matrix reveals.  It may be the case that in this relatively small example (\ie, relatively few documents) we lack enough text to exploit asymmetry in the use of words.
 
 
 \begin{figure}
 \caption{ \label{fig:cca} { \sl Canonical correlations between the 500 columns of $C_l$ and $C_r$.}}

 \centerline{
 \vspace{0.1in}
 \includegraphics[width=3in]{figures/cca}  }
 \vspace{0.2in}
 \end{figure}


 \begin{figure}
 \caption{ \label{fig:regrBcca} { \sl Canonical variables from the analysis of the dependence between $C_l$ and $C_r$ concentrate more signal into the leading regressors.}}

 \centerline{
 \vspace{0.1in}
 \includegraphics[width=5in]{figures/regrBcca}  }
 \vspace{0.2in}
 \end{figure}


How well does combining both sets of variables perform?  To find out, we start with the variables from the LSA, the singular vectors of $W$.   The regression on the 500 singular vectors in $U_W$ explains $\ol{R}^2 = 0.612$ of the variation in log prices.  Adding the information from 500 more columns in $C_l$ boosts the total to $\ol{R}^2 = 0.679$.  Adding the remaining variation from $C_r$ raises the total slightly (albeit significantly) to $\ol{R}^2 = 0.703$. Interestingly,  the original parsed variables (Table \ref{tab:parsed}) offer ever so slightly more predictive power. The improvement only adds 0.003 to $\ol{R}^2$, but this is highly significant ($F$=12.1).


%--------------------------------------------------------------------------
\section{Probability models}
\label{sec:topicmodels}
%--------------------------------------------------------------------------

 To provide some explanation for the evident success of this direct approach to building regressors from text, we offer a hypothetical data generating process for text and study the implications of this DGP for regression modeling.  The DGP is essentially that used in topic modeling \citep{blei12}.  In machine learning, topic modeling is an unsupervised technique that  clusters documents based on the presence of shared, underlying ``topics'' revealed by a hierarchical Bayesian  model.  Our method of featurizing text is also unsupervised,  but we seek to predict an explicit response rather than uncover latent clusters.  Nonetheless, we can study how our procedure would perform were there an underlying topic model.


\subsection {Topic Models}  % -------------------------------------------

 We begin with the simplifying assumption that real estate properties possess
 varying amounts of $K$ unobserved traits that influence both the value of
 a property as well as the language used to describe the property.  For example,
 such traits might include the quality of construction, presence of renovations,
 proximity to desirable conveniences and so forth.  In the context of topic models, these traits define the underlying topics shared by an ensemble of documents.  In what follows, the subscript $i$ indexes documents ($i = 1,\ldots,n$), $m$ denotes words ($m=1,\ldots,M$), and $k$ indexes traits ($k = 1,\ldots,K$).  Recall that words are tokens identified in the preprocessing of the text, not words in the usual sense.   Let $y = (y_1,\ldots,y_n)'$ denote the column vector that holds the response that is to be modeled by regression analysis.    In our application, $y$ is the vector of the log of prices of real estate. (All vectors are column vectors.)


 Within this model, traits influence the response via a familiar regression
 equation.  The connection between traits and documents is given by an unobserved $n \times K$ matrix of latent features $\zeta = [\zeta_{ik}]$.  Each row of $\zeta$ defines a mixture of traits that defines the distribution of words that appear in each document.  To avoid further notation, we use subscripts to distinguish the rows and columns of $\zeta$.  The vector $\zeta_{i*}$ identifies the row of $\zeta$ associated with document $i$, and $\zeta_{*k}$ identifies the column of $\zeta$ associated with topic $k$:
 \begin{displaymath}
   \zeta = \left( \begin{array}{c}
                    \zeta_{1*}' \cr \zeta_{2*}' \cr \vdots \cr \zeta_{n*}'
                  \end{array}
           \right) 
         = \left( \zeta_{*1} \; \zeta_{*2} \; \cdots \; \zeta_{*K} \right).
 \end{displaymath}
 $\zeta_{i*}$ specifies the distribution of traits present in the $i$th real-estate property; $0 \le \zeta_{ik} \le 1$ with $\sum_k \zeta_{ik} = 1$.  We assume that the allocation of traits within each document is an independent realization of a Dirichlet random variable, 
 \begin{equation}
  \zeta_{i*} \sim \mbox{Dir}(K, \al_K) \;,
  \label{eq:zeta}
\end{equation}
where $\al_m$ denotes the $K$-dimensional parameter vector of distribution.    Given $\zeta$, the $K$ traits  influence the response through a linear equation of the familiar form
 \begin{equation}
    \ev y_i = \zeta_{i*}' \, \beta \;.
 \label{eq:regr}
 \end{equation}
 The coefficients $\beta$ determine how the traits influence the response.  


These traits also determine the distribution of words that appear in documents.  This connection allows us to recover $\zeta$ --- which is not observed --- from the associated text.  Assume that a trait defines a probability distribution over the word types in the vocabulary $V$.  Let $P_k$ denote the distribution of word-types used when describing trait $k$; in particular, $P_{km}$ is the probability of using word type $m$ when describing trait $k$.  Our DGP models these distributions over word types as another set of independent Dirichlet random variables,
 \begin{equation}
   P_k \sim \mbox{Dir}(M, \al_M),
   \label{eq:Pk}
 \end{equation}
where $\al_M$ is the $M$-dimensional parameter vector for the distribution.
 Collect these discrete
 distributions in the $K \times M$ matrix
 \begin{equation}
    P = \left(  \begin{array}{c} 
                    P_1' \cr P_2' \cr  \vdots \cr P_K'
                \end{array}
        \right) \;.
 \label{eq:P}
 \end{equation}
 
 
The Dirichlet variables $\zeta$ and $P$ together determine a distribution for the counts of words that appear in each document (its bag-of-words).  First, we assume that the number of words in each document is another independent random variable, and for our simulation we use a negative binomial, formed by mixing Poisson distributions with parameters that have a Gamma distribution,
\begin{equation}
  m_i|\la_i \sim \mbox{Poisson}(\la_i), \quad \la_i \sim \mbox{Gamma}(\al),
\end{equation}
independently over documents.  To `construct' the $i$th document from this model, we sample $m_i$ words from the underlying $K$ topics by the following mechanism.  Let $w_{im}$ denote the $m$th word in the $i$th document.  For this word, pick a topic at random (and independently) from the topic distribution identified by $\zeta_{i*}$, say $k_{im} \sim \mbox{Multi}(\zeta_{i*})$.  Then choose $w_{im}$ from the multinomial distribution with these probabilities, 
\begin{equation}
  w_{im} \sim \mbox{Mult}(P_{k_{im}}), \quad i = 1,\ldots,m_i \;.
  \label{eq:wim}
\end{equation}
Hence, the vector of counts $w_i$ for the $i$th document has a multinomial distribution whose parameters are determined by its mixture of traits:
 \begin{equation}
   w_i' \sim \mbox{Multi}(m_i, \zeta_i' P)   
 \label{eq:di}
 \end{equation}
 implying that $\ev w_i' | m_i = m_i \; \zeta_i'P$.  
 
 \remark{ According to this DGP, the length of a document $m_i$  does not affect the response; only the mixture of traits is relevant.  Our results with real text summarized in the regression (Table \ref{tab:parsed}) provide contradictory evidence: Document length has a significant impact on price. }
 
 
   Given that documents are generated by a topic model defined by equations \eqn{regr} -- \eqn{di}, the challenge for making regression features from text is to recover the $K$-dimensional linear space spanned by $\zeta$.  The success of latent semantic analysis (LSA, principal components analysis of the word counts $W$) is particularly easy to see. Intuitively, LSA is well-matched to this DGP because both treat a document as a bag-of-words.  Let $D_m$ denote an $n \times n$ diagonal matrix with the  word counts $m_i$ along the diagonal.  Then the expected value of the document/word matrix $W$ is the sum of $K$ outer products:
\begin{equation}
    \ev W = D_m \; \zeta \; P = D_m \sum_k \zeta_{*k} P_k' \;.
  \label{eq:EW}
\end{equation}
The expectation factors as an outer product, just as an SVD represents a matrix. That is, if we write $X = UDV'$, then we can express the matrix product as the sum $X = \sum_j d_{jj} u_j v_j'$ where $u_j$ and $v_j$ are the columns of $U$ and $V$, respectively.  For our models of text, the left singular vectors $U_W$ from \eqn{svdW} are related to the allocation of traits over documents held in $\zeta$.  Of course, there are many ways to factor a matrix, and it is not apparent why the factorization provided by the SVD would be better than others.  Our rationale relies on convenience (and the evident success in modeling real-estate prices), but one can argue that a decomposition that yields positive factors, namely non-negative matrix factorization NMF, would be more appropriate. Because both $\zeta$ and $P$ are probability matrices, a constrained optimization that factors $W$ into matrices whose rows are discrete probability distributions would be ideal.   We do not explore these here. 


\remark{ Expression \eqn{EW} suggests that we should factor out $D_m$ from $W$ before doing the singular value decomposition so that variation of the lengths $m_i$ does not contaminate the left singular vectors.  This replacement of counts by proportions would then be followed by weighted least squares to down weight the influence of short documents.  We explored several variations of this weighting, but none made a dramatic difference in the goodness of fit or out-of-sample accuracy. }


The connection of this DGP to bigrams is less obvious and relies more on stronger assumptions.  Bigrams count the frequency of the adjacent word types, a property we associate with the sequence of words rather than co-occurrence within a document.  To see how the analysis of bigrams can nonetheless produce useful regressors, we need to add either stronger assumptions to our DGP or incorporate some type of sequential dependence.  For example, we might assume that words associated with traits appear in {\em phrases}.   As in the bag-of-words model, words within a phrase are drawn independently from the distribution defined by a trait, but the generating process samples within a trait for some length of time before transitioning to another trait (resembling an HMM).  If these phrases are relatively long, then we can approximate the expected counts in the bigram matrix as a weighted outer product of the probability vectors for the traits.  We can obtain the same heuristic in our DGP by assuming that the probability distributions $P_k$ that define the traits have (nearly) singular support on words.  That is, most words are associated with a unique trait (implying $P_{k_1}'P_{k_2}  \approx 0$).  In either case, the marginal probability of finding adjacent words types reveals the underlying probability distribution.  


For instance, suppose the traits have disjoint support on words and that documents have common length $m_i \equiv m$. Then the probability of finding  word types  $w_{m_1}$ and $w_{m_2}$ from trait $k$ adjacent to each other is 
 \begin{equation}
  P(w_{m_1},w_{m_2}) = \sum_i \left(\zeta_{ik}^2/n \right) P_{km_1} P_{km_2} 
                                     = h_k P_{km_1} P_{km_2}  \;.
  \label{eq:joint}
\end{equation}
Let $N = \sum m_i$ denote the total number of observed words. Using the expression \eqn{eq:joint}, the expected value of the bigram matrix factors as
 \begin{equation}
    \smfrac{1}{N} \ev B \approx \sum h_k P_k\; P_k' = P' H P \;, 
                     \qquad  H = \mbox{diag}(h_k)\;,
 \label{eq:B2}
 \end{equation}
Again, a constrained factorization that produced probability vectors would be more appropriate if one truly believed this model.  In an ideal world, the singular values of the SVD would capture the unobserved marginal probabilities  $h_k$.  Expression \eqn{B2} also suggests why the left and right singular vectors of $B$ should match, or at least define a common subspace.


 The factorization of $B$ defines the coordinates of eigenwords ($\R^M$).  To
 obtain coordinates in document space ($\R^n$) for use in regression, we  correlate  the word counts in $W$ with the eigenwords.  For the $i$th description, if we pretend that the factorization of $B$ is exact and approximate the word counts $w_i$ by their expectation, then the first column of $w_i' U_b$ is
 \begin{displaymath}
    w_i'P_1 \approx m_i \zeta_i' P P_1 \;.
 \end{displaymath}
 If the probability distributions of the traits are roughly singular as argued previously, then
 \begin{displaymath}
    w_i'P_k \approx m_i \zeta_{i1} P_1'P_1 \;.
 \end{displaymath}
 Hence, to this rough approximation, the correlation between $w_i$ and the first
 left singular vector of $B$ is
 \begin{displaymath}
    \corr(w_i, U_{B1}) = \frac{\zeta_{i1}}{(\zeta_{i*}'\zeta_{i*})^{1/2}}   
 \end{displaymath}
 The L2 normalization is just right for canceling $P_1$, but leaves a constant
 factor $\norm{\zeta_i}$.

 
 Of course, even in expectation, the factorization of the bigram $B$ will not
 match  \eqn{eq:B2}; the SVD  only recovers a basis for the range of $B$.  Thus, the 
 singular vectors will mix the probability vectors.  We can
 then think about $U_B = P'O$ for some orthonormal matrix $O$.  That is, ideally
 the singular vectors span the correct space, but in a different basis so that
 we observe (again, in expectation)
 \begin{displaymath}
   w_i'U_b \approx (m_i \zeta_i'P)(P'O) = m_i \zeta_i \mbox{diag}(P_k'P_k) O
 \end{displaymath}
 Hence, when computing the correlations between the observed counts $w_i$ and the
 singular vectors, the norm of the probability distributions cancel and we
 obtain a rotation of  $\zeta_{i*}$ vector.  The rotation is the same, however,
 for all rows, and consequently our collection of regressors spans the same
 space as the unrotated $\zeta$.


 Obtaining a the relevant $\zeta$-coordinates for a new document is routine in
 this case.  One simply mimics the process used to identify/estimate $\zeta$ for
 the observed cases by correlating the counts for the new description, say
 $w_{new}$, with the matrix defined by the eigenwords.



%--------------------------------------------------------------------------
\subsection{Examples: Simulated Data from Topic Models}

 To get a better handle on how the singular value decomposition works, consider
 a simulated world in which a topic model generates text from a vocabulary of $M
 = 2,000$ words types.  Assume that words in $n = 6,000$ documents are generated by mixture of $K=10$ topics, with each topic defined by  one of $K$ distributions $P_1,\ldots, P_{10}$ over the 2,000 word types.  
 Hence, $p_1$ lies in the $M$-dimensional
 simplex.     Similarly, the distribution of topics within the $i$th document is distributed as $\zeta_{i*} \sim \mbox{Dirichlet}(K, \al_k=0.1)$.  The response for a document is a weighted sum of the mix of topics within the document.  We consider two situations, first the idealized case in which topic distributions are essentially disjoint, and then with topic distributions share common words.
 
 \subsubsection{Nearly Disjoint Topic Distributions} % -----------------
 
  
 For this example, we simulate $P_k$ by independently drawing from a Dirichlet distribution with parameters $M$ and $\al_m = 0.05$ for all $m = 1,\ldots,M$.  Small values of $\al_m$ lead to `spiky' distributions with little overlap.  For example, Figure \ref{fig:simdist}(a) graphs the probabilities assigned by two of the 10 distributions for this first simulation.  The defining constants for this first example are:
 \begin{displaymath}
    M = 2000 \mbox{ word types}, \quad
    n = 4000 \mbox{ documents},  \quad
    K = 10 \mbox{ topics}   
 \end{displaymath}
 with random variables defined by
 \begin{eqnarray*}
    m_i           &\sim& \mbox{Pois}(\la_i), \la_i \sim \mbox{Gamma}(30,1)\;,
                     i = i,\ldots, n, \quad \mbox{ (Negative Binomial) }       \cr
    P_k                &\sim& \mbox{Dir}(M, \al_m=0.05), \quad k=1,\ldots,K,  \cr
    \zeta_{i*}        &\sim& \mbox{Dir}(K, \al_k=0.10),\quad i=1,\ldots,n,  \cr
    k_{im}             &\sim& \mbox{Mult}(\zeta_{i*}), \quad m=1,\ldots,m_i,    \cr 
    w_{im}|k_{im} &\sim& \mbox{Mult}(P_{k_{im}}), \quad i=1,\ldots,n \;, \cr
    y_i                  &\sim& N(\zeta'\beta, 0.5^2), \quad i = 1,\ldots,n \;.
 \end{eqnarray*}
 With these choices, $\ol{R}^2 \approx 0.92$ for the regression of $y$ on $\zeta$.

 
\begin{figure}
 \caption{ \label{fig:simdist}
  \sl Probabilities assigned to words by two simulated topic distributions with (a) nearly disjoint support and (b) with overlapping words.}  
  \centerline{   
     \includegraphics[width=3in]{figures/simdist}    
     \includegraphics[width=3in]{figures/simdistB}    }
\end{figure}


The distribution of words produced by this topic model  loosely resembles the distribution found in real estate ads.  Compare Figure \ref{fig:simzipf}(a) to Figure \ref{fig:zipf} from the real estate data.  Though a good match for the less common words, the most common words are not as common as one might want.  The common words do not have a high enough frequency (disjoint topics lack common words like `a' and `the' and punctuation as found in the real-estate listings shown in Figure \ref{fig:zipf}), and the frequencies for rare words drop off too quickly.  


\begin{figure}
\caption{ \label{fig:simzipf} { \sl Distribution of word frequencies for simulated
topic data, based on (a) disjoint word distributions and (b) over-lapping distributions.}}
 \centerline{
 \vspace{0.1in}
 \includegraphics[width=2.5in]{figures/simzipfA}  
 \includegraphics[width=2.5in]{figures/simzipfB} }
  \vspace{0.2in}
 \end{figure}


Table \ref{tab:simregr} summarizes the fits using $k_W = 100$ singular vectors and $k_B=100$ left and right singular vectors of $B$.  The fits recover much, but certainly not all of the underlying regression structure (for which $R^2 \approx 0.92$).  The fitted models produce similar fitted values, as shown in Figure \ref{fig:simfits}.  The two fits, however, deviate for documents with either lower or higher values of $y$. 

% \ras{ Would measurement error produce this tail curvature?}

\begin{table} 
\caption{ \label{tab:simregr}
\sl Fits to regression in simulated topic data using singular vectors of $W$ and $B$ for two topic distributions.}
\begin{center} 
\begin{tabular}{|c|ccc|}
\hline
  Topic Structure & Num Regressors & Origin & $\ol{R}^2$ \cr \hline \hline
   Disjoint            &         100              &  $W$  &  0.746        \cr
                           &         200              &  $B$  &   0.788        \cr \hline \hline
   Overlapping    &          100              &  $W$ &   0.516       \cr
                           &          200             &  $B$  &   0.626        \cr \hline
\end{tabular}
\end{center}
\end{table}


\begin{figure}
\caption{ \label{fig:simfits}  
{\sl Scatterplot of fitted values for the two regressions based on singular vectors of $W$ and $B$.}  The shown smooth curve is the lowess fit, and the correlation $r \approx 0.97$.}
\centerline{  \includegraphics[width=4in]{figures/simfits}  }
\end{figure}


Because the topic distributions are nearly disjoint, one can identify the number of topics $K$ from the singular value decompositions.  The value of $K$ is most apparent in a canonical correlation analysis of the singular vectors of $B$.  Figure \ref{fig:simccab} graphs the canonical correlations for the left and right singular vectors
of the bigram matrix for the simulated text.  The clear break in the sequence of singular vectors confirms the presence of $K=10$ topics.  

 \begin{figure}
  \caption{ \label{fig:simccab} { \sl Canonical correlations for left and right
 singular vectors of the bigram matrix $B$ of simulated topic data.} (a) The underlying topics are nearly disjoint in support. (b) The underlying topics overlap.}
\vspace{0.1in}
 \centerline{
      \includegraphics[width=3in]{figures/simccab}  
      \includegraphics[width=3in]{figures/simccabb}   }
  \end{figure}
 
 
 \subsubsection{Overlapping Topic Distributions} % -----------------
 
 The ability to recover the underlying regression structure is diminished in the presence of overlapping topics.  The simulation in this section is virtually identical to the previous one, but for overlapping topics.  The simulation again has a vocabulary of $M=2,000$ words, $n=6,000$ documents, and $K=10$ topics.  Rather than simulate the topics distributions independently, however, we introduce a common structure.  The topic distributions share 150 ``common words'' that follow with probabilities from a Dirichlet distribution.  Let $X \sim \mbox{Dir}(150, 2)$ and let $\xi$ denote the vector obtained by concatenating $M$ - 150 zeros onto $X$, $\xi' = (X, 0 , \ldots, 0)$.  The distribution that defines the $k$th trait is then $P_k = \xi/2 + Z/2$ with $Z \sim \mbox{Dir}(\al_m)$.  Figure \ref{fig:simdist}(b) plots the probabilities of two distributions generated in this manner.  The diagonal points show the substantial overlap.  The number of topics per document and the marginal distribution of words per document are both similar to the prior simulation and are not materially influenced by the introduction of this overlap.
 
 
 The same cannot be said for the regressors produced from singular value decompositions.  The lower portion of Table \ref{tab:simregr} summarizes these fits.  The regression on 100 singular vectors from $W$ now captures $\ol{R}^2 = 0.516$ of the variation in the response, and the regressors from the bigram obtain $\ol{R}^2 = 0.626$.  The correlation between fitted values falls to 0.86 (down from 0.97 when no overlap was introduced).  Curiously, the bigram is less influenced by the overlap.  
 Canonical correlation of the singular vectors of $B$ also no longer reveals the number of topics (Figure \ref{fig:simccab}(b)).   The canonical correlations between left and right singular vectors of $B$ now provide no hint as to the choice of $K$.  
 
 
 Figure \ref{fig:simregr} summarizes the statistical significance of the coefficient estimates in this model.  The distribution of significance is rather different from that seen in the real estate models (Figure \ref{fig:svdregr}).  For example, statistical significance decays in the real estate model as one moves from the leading singular vectors of $W$ down to smaller vectors.  The simulated results show no hint of greater significance in the leading singular vectors and have a diffuse signal.
  Also, both half-normal plots are very nearly linear, with little of the ``superstar'' character seen in the $t$-statistics from the regression models for real estate.  What is similar to the model for prices is that the signal is more diffuse in the regression using correlation regressors derived from $B$.  The slope of the line fitted to the half-normal plot of the $t$-statistics for the singular vectors of $W$ is 8.8, whereas that for the regressors derived from $B$ is 3.6.  
 
 
 \begin{figure}
 \caption{ \label{fig:simregr}
  {\sl Distribution of statistical significance in the regression with simulated, overlapping topic models.} Results first for the regression on singular vectors from $W$, then those derived from the bigram matrix $B$.}
   \centerline{ \includegraphics[width=5in]{figures/simregrW}  }
   \centerline{ \includegraphics[width=5in]{figures/simregrB}  }
 \end{figure}

%--------------------------------------------------------------------------
\section{Variable Selection and Cross Validation}
\label{sec:cv}
%--------------------------------------------------------------------------

The previous examples routinely estimate regression models with hundreds of explanatory variables.  Though large by traditional standards, these models do not suffer from the problems associated with over fitting because we have not used the data to pick the model.  We simply fit a large collection of regressors.  Evaluating such models is thus the province of classical statistical tests and criteria such as adjusted r-squared.  

As evidence that these models are not over fit, we offer the following example.  Consider the LSA regression that uses 500 principal components of $W$.  Adjusting for the effects of estimating the 500 coefficients and the intercept anticipates the out-of-sample mean squared error of prediction to be $s_e^2 (1+(p+1)/n)$.  This simple approximation averages over the regression design, ignoring leverage effects.  Plugging in the unbiased estimate $s_e^2 = 0.5649$ gives $0.5649 (1+501/7384) = 0.603$.  Because the values of the regressors in the test data do not match those in the training data, this estimator is typically smaller than the observed MSE by an amount depending on variation in the regressors.  

For comparison, we performed repeated 10-fold transductive cross validation.  Transductive cross-validation presumes that the full collection of regressors is available for the training data, which in this case implies that we have all of the data available to perform the principal components analysis.  Only the values of the response are hidden in each fold of the cross-validation; the collection of regressors is fixed.  We repeated the 10-fold cross-validation 20 times, each time randomly partitioning the cases into 10 folds. The observed average squared error was slightly higher than anticipated at $0.614 \pm 0.007$, but basically agreed with the estimate from the fitted model. 


%--------------------------------------------------------------------------
\section{Lighthouse Variables and Interpretation}
\label{sec:light}
%--------------------------------------------------------------------------
 
 

 Our emphasis on predictive accuracy does not necessarily produce an
 interpretable model, and one can use other data to create such structure.  Our
 explanatory variable resemble those from principal components analysis and
 share their anonymity.  To provide more interpretable regressors, the presence
 of partial quantitative information in real estate listings (\eg, some listings
 include the number of square feet) provides what we call ‘lighthouse variables’
 that can be used to derive more interpretable variables.  In our sample, few
 listings (about 6\%) indicate the number of square feet.  With so much missing
 data, this manually derived predictor is not very useful as an explanatory
 variable in a regression.  This partially observed variable can then be used to
 define a weighted sum of the anonymous text-derived features, producing a
 regressor that is both complete (no missing cases) and interpretable.  One
 could similarly use features from a lexicon to provide more interpretable
 features.


Though regression models are seldom causal, one is often tempted to interpret
 properties of the solution within the domain of application.  Because the
 predictors computed from the decompositions in the prior section describe
 subspaces rather than some specific property of words, interpretation is
 essentially non-existent.


 To obtain features that are interpretable, we exploit the pre<sence of
 characteristics that are occasionally observable.  For example,
 most home listings do not include the number of bathrooms.  An
 explanatory variable obtained by parsing this count is missing for 74\% of the property listings.  We can use this partially observed variable, however, to construct a more interpretable variable from either the principal components of $W$ or the bigram variables.  


 Let $z \in \Rn$ denote the partially observed or perhaps noisy data
 that measures a substantively interesting characteristic of the
 observations.  For our example, $z$ is the partially observed count of the number of bathrooms.  Rather than use $z$ directly as a predictor of $y$, we
 can use it to form an interpretable blend of, for instance, $U_W$.   In
 particular, we simply regress $z$ on these columns, finding the
 linear combination of these basis vectors most correlated with the
 observed variable.  This variable, call it $\hat{z}$ then becomes
 another regressor.  Because such variables can be used to guide the
 construction of interpretable combinations of the bases $U_W$ and
 $C$, we call these lighthouse variables.  
 
 
 In our example, the correlation between the number of bathrooms and the price of the listing is 0.42 for listings that show this characteristic.  This correlation is much smaller (0.19) if we fill the missing values with the mean number of bathrooms (Figure  \ref{fig:parsed}).  If we form the projection $\hat{z}$ given by regressing the observed counts on the corresponding rows of $U_W$, this new regressor has correlation 0.29 with the log of price.



%--------------------------------------------------------------------------
\section{Summary and Next Steps}
\label{sec:disc}
%--------------------------------------------------------------------------
  
  Regression modeling always benefits from greater substantive insight, and the modeling of text is no exception.  An obvious approach to building regressors from text data relies on a
 substantive analysis of the text.  For example, sentiment analysis constructs a
 domain-specific lexicon of positive and negative words.  In the context of real
 estate, one might suspect  words such as ``modern'' and ``spacious''  to be associated with more expensive properties, whereas
 ``Fannie Mae'' and ``fixer-upper'' to signal properties with lower prices.  The
 development of such lexicons has been an active area of research in sentiment
 analysis over the past decade \citep{taboada11}.  The development of a lexicon
 require substantial knowledge of the context and the results are known to be
 domain specific.  Each new problem requires a new lexicon.  The lexicon for
 pricing homes would be quite different from the lexicon for diagnosing patient
 health.  Our approach is also domain specific, but requires little user input
 and so can be highly automated.


 Our analysis here shows that one can exploit well-known methods of multivariate analysis to create regressors from unstructured text.  Compared to iterative methods based on MCMC, the computations are essentially immediate.  Surprisingly, the resulting regressors are quite predictive in several examples we have explored.  For example, we used this same methodology to model ratings assigned to wines based on tasting notes.  The tasting notes themselves are typically shorter than these real estate listings (averaging about 42 tokens compared to 72 for the listings), but we have a larger collection of about 21,000.  Using the methods demonstrated here, a regression using 250 principal components of $W$ explains about 66\% of the variation in ratings, we a remarkably similar distribution of effect sizes as shown in Figure \ref{fig:wine}.  Similarly, regression on the 250 left and 250 right regressors constructed from the bigram matrix explains about 68\% of the variation.  We plan to explore other applications in the near future.
 
 
 \begin{figure}
 \caption{ \label{fig:wine} \sl Regression t-statistics from a model that predicts wine ratings using 250 principal components of $W$ based on 21,000 wine tasting notes.}
 
 \centerline{
   \includegraphics[width=4in]{figures/wine.pdf}
   }
  \end{figure} 
  
  
   The connection to topic models is an important aspect of these results.  Topic models define a DGP for which the regressors that we construct capture the underlying data-generating mechanism.  If one accepts topic models as a reasonable working model of the semantics of text, then it is no accident that regressors constructed from text are predictive.


 Our work here merely introduces these methods, and we hope that this introduction will encourage more statisticians to engage problems in  modeling text.  Our results here also suggest several directions for further research:

   \begin{description}
   
   \item[n-grams.]  Our example uses bigrams to capture word associations captured by adjacent placement.  Other reasonable choices define different measures of context,  such as trigrams (sequence of 3 words) or skipped bigrams (words separated by some count of tokens).  Some preliminary results show, for instance, that trigrams offer modest gains, albeit at a nontrivial increase in computation.
   
   \item[Transfer learning.]  Transfer learning refers to learning what can be extrapolated from one situation to another.  In our context, it would be of interest to learn how well models developed from data in June 2013 work when applied to data from later time periods or different locations.  It is evident that the models shown here would not perform so well applied to the language of a different market, such as in Miami or Los Angeles.  Not only do the characteristics of valuable properties change, but  local conventions for phrasing listings are also likely to be different.  Having a methodology for distinguishing idiosyncratic local features from those that generalize in time or space would be valuable. 
   
  \item[Alternative forms of tokenization.] Would be interesting to explore the use of stemming to reduce the number of word types and with a larger collection of documents, to explore annotation (that would distinguish words by their part of speech).  Further parsing, lexical analysis.  Some readers will be troubled by the simplicity of the  bag-of-words representation of a document.  Our methods understand neither the English language nor the rules of grammar and spelling.  They do not attempt to untangle  multiple uses of the same word.  Linguists have debated the ability of such  representations to reveal the meaning of language, and it is clear that the bag of words representation loses information.  Just imagine cooking from``bag of words'' recipe or following a ``bag of words'' driving directions.  Nonetheless, this very direct representation produces very useful explanatory variables within our application.  We leave open the opportunity to embellish this approach with more domain specific methods of parsing, such as adding part-of-speech tags and lexical information.
   
   \item[Use of unsupervised data.] Most words are used only once or twice, meaning that we lack enough data to identify their connection to the response or indeed to other words.  As a partial remedy, it may be possible to build regressors that represent such words from larger, more generic text such as the collection of n-grams collected by Google.  Using this supplemental unsupervised data requires solving the problem of transfer learning, at least to some degree, but opens the door to much more extensive examples. 
   
   \item[Variable selection.]  The distribution of effects (such as shown by the $|t|$ statistics of the text-derived regressors) are poorly matched to the so-called `nearly black' model commonly adopted in research on the theory of variable selection.  Rather than have most of the predictive power concentrated in a very small number of regressors, these regressors spread the power of many.  
   
   It would also be interesting to explore these models for nonlinearity. Variable selection is perhaps unnecessary for using the regressors derived from $U_W$ and $C$, but essential if one hopes to detect and incorporate nonlinearity. In particular, searching for nonlinearity -- such as interactions -- requires variable selection.  Even a very large corpus of documents looks small compared to the number of possible second-order interactions.  Finally, the ordered presentation of the $|t|$ statistics suggests an opportunity for variable selection derived from alpha investing \citep{fosterstine08}.  Alpha investing is a procedure for testing a sequence of hypotheses that benefits from {\it a priori} ordering of the tests.
   
   \end{description}

One can generalize LSA from words and documents to collections of other items,
 such as phonemes in speech or tones in music \cite[called latent semantic
 mappingin][]{bellegarda05}.



%--------------------------------------------------------------------------
\section*{Acknowledgement}
%--------------------------------------------------------------------------

The authors thank Vanessa Villatoro from Trulia's PR Team for allowing us to scrape the data from their web site.


%--------------------------------------------------------------------------
% References
%--------------------------------------------------------------------------

\bibliography{../../../references/stat,../../../references/TextPapers/text}
\bibliographystyle{../bst/ims}

\end{document} %==========================================================
